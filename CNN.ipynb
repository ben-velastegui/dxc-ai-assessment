{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPd/zod5qZA6ia2JNWTfto",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben-velastegui/dxc-ai-assessment/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "UCwpGwbjule0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Deep learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Feature importance\n",
        "!pip install shap\n",
        "import shap\n"
      ],
      "metadata": {
        "id": "p0axdUyGuj9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/full_data_long.csv\")\n"
      ],
      "metadata": {
        "id": "uxe4vnusuiWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = [\n",
        "    \"total_logs\", \"error_logs\", \"warning_logs\", \"info_logs\",\n",
        "    \"cpu_user_mean\", \"cpu_user_max\",\n",
        "    \"mem_used_mean\", \"mem_used_max\",\n",
        "    \"load1_mean\", \"load5_mean\", \"load15_mean\",\n",
        "    \"total_traces\", \"missing_data\"\n",
        "]\n",
        "\n",
        "\n",
        "categorical_features = [\"Hostname\", \"program\", \"pid\", \"user_id\"]\n"
      ],
      "metadata": {
        "id": "msWYZvv5ug9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"operation\"\n"
      ],
      "metadata": {
        "id": "zst18F8Bufb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_column = \"bin_time\"\n"
      ],
      "metadata": {
        "id": "lSJmmFuwueYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n"
      ],
      "metadata": {
        "id": "l4LfAGfmuc4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Corrected for newer scikit-learn\n",
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "cat_encoded = ohe.fit_transform(df[categorical_features])\n",
        "\n",
        "# Combine numeric + categorical features\n",
        "df_encoded = pd.concat([\n",
        "    df[numeric_features].reset_index(drop=True),\n",
        "    pd.DataFrame(cat_encoded, columns=ohe.get_feature_names_out(categorical_features))\n",
        "], axis=1)\n"
      ],
      "metadata": {
        "id": "DG3kiV7Nubki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "df[\"operation_label\"] = label_encoder.fit_transform(df[target])\n",
        "y = to_categorical(df[\"operation_label\"])\n"
      ],
      "metadata": {
        "id": "YzGVCY84uaCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert bin_time to datetime, filling missing time as 00:00:00\n",
        "df[\"bin_time\"] = pd.to_datetime(df[\"bin_time\"], errors=\"coerce\", infer_datetime_format=True)\n",
        "\n",
        "# For entries that were just dates, pandas will automatically set time to 00:00:00\n",
        "# Verify\n",
        "print(df[\"bin_time\"].head())\n",
        "\n",
        "# Sort by Hostname and bin_time\n",
        "df = df.sort_values(by=[\"Hostname\", \"bin_time\"]).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "VzBf2C0DuYr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 10  # number of time steps per sequence\n",
        "\n",
        "X_sequences = []\n",
        "y_sequences = []\n",
        "\n",
        "for host in df[\"Hostname\"].unique():\n",
        "    host_data = df[df[\"Hostname\"] == host]\n",
        "    host_features = df_encoded.loc[host_data.index].values\n",
        "    host_labels = df.loc[host_data.index, \"operation_label\"].values\n",
        "\n",
        "    for i in range(len(host_features) - sequence_length + 1):\n",
        "        X_sequences.append(host_features[i:i+sequence_length])\n",
        "        y_sequences.append(host_labels[i+sequence_length-1])  # label at last timestep\n",
        "\n",
        "X_sequences = np.array(X_sequences)\n",
        "y_sequences = np.array(y_sequences)\n",
        "y_sequences_cat = to_categorical(y_sequences)\n",
        "\n",
        "print(\"Sequences shape:\", X_sequences.shape)\n",
        "print(\"Labels shape:\", y_sequences_cat.shape)\n"
      ],
      "metadata": {
        "id": "0RbALSeguXGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_sequences, y_sequences_cat, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test shape:\", X_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "id": "WJ9ooDI8uVwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, Dense, Flatten, Dropout, BatchNormalization\n",
        "\n",
        "input_shape = X_train.shape[1:]  # (sequence_length, num_features)\n",
        "\n",
        "inputs = Input(shape=input_shape)\n",
        "x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(y_sequences_cat.shape[1], activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "GWtiH1ofuUWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=20,\n",
        "    batch_size=32\n",
        ")\n"
      ],
      "metadata": {
        "id": "IixH8lvpuSs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure operation column is clean before encoding\n",
        "df[target] = df[target].astype(str)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"operation_label\"] = label_encoder.fit_transform(df[target])\n",
        "y = to_categorical(df[\"operation_label\"])\n"
      ],
      "metadata": {
        "id": "SQ2RwmbwuRE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace np.nan or string \"nan\" with \"unknown\"\n",
        "df[target] = df[target].replace(\"nan\", np.nan)  # catch string nan\n",
        "df[target] = df[target].fillna(\"unknown\").astype(str)\n",
        "\n",
        "# Refit label encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"operation_label\"] = label_encoder.fit_transform(df[target])\n",
        "y = to_categorical(df[\"operation_label\"])\n",
        "\n",
        "print(\"Classes:\", label_encoder.classes_)\n"
      ],
      "metadata": {
        "id": "w8iD4BbNuOm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Use integer labels, not one-hot\n",
        "class_weights_array = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(df[\"operation_label\"]),\n",
        "    y=df[\"operation_label\"]\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights_array))\n",
        "\n",
        "print(\"Class weights:\", class_weights)\n"
      ],
      "metadata": {
        "id": "sO9JYSxZuNSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights\n",
        ")\n"
      ],
      "metadata": {
        "id": "vgv6tK4quMEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Report\n",
        "print(classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cjvbkXgcuKg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import numpy as np\n",
        "\n",
        "# Flatten sequences for oversampling\n",
        "X_flat = X_sequences.reshape(len(X_sequences), -1)\n",
        "y_flat = y_sequences  # integer labels, not one-hot\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_flat, y_flat)\n",
        "\n",
        "# Reshape back into sequence form\n",
        "X_resampled = X_resampled.reshape(-1, sequence_length, X_sequences.shape[2])\n",
        "y_resampled_cat = to_categorical(y_resampled)\n",
        "\n",
        "print(\"Original class distribution:\", np.bincount(y_flat))\n",
        "print(\"Resampled class distribution:\", np.bincount(y_resampled))\n"
      ],
      "metadata": {
        "id": "DEu5mx0xuGA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_resampled, y_resampled_cat,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "UO_0RYjHuEMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Predictions shape:\", y_pred_classes.shape)\n",
        "print(\"True labels shape:\", y_true.shape)"
      ],
      "metadata": {
        "id": "6mLX0lS9uCpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "target_names = [str(c) for c in label_encoder.classes_]\n",
        "\n",
        "report = classification_report(y_true, y_pred_classes, target_names=target_names, zero_division=0)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "JXLNTwMTuBEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=target_names,\n",
        "            yticklabels=target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EepW3j19t_jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "for i, class_name in enumerate(target_names):\n",
        "    idx = y_true == i\n",
        "    class_acc = np.sum(y_pred_classes[idx] == i) / np.sum(idx)\n",
        "    print(f\"Accuracy for {class_name}: {class_acc:.2f}\")"
      ],
      "metadata": {
        "id": "7aKZzB8Qt90q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='val')\n",
        "plt.title(\"Accuracy over epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='val')\n",
        "plt.title(\"Loss over epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IpKiE9BYt0DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this explains why you’re still seeing all predictions as boot_delete. The model literally did not learn anything for the minority classes, which causes both:\n",
        "\t1.\tPrecision/recall = 0 for all non-majority classes.\n",
        "\t2.\tSHAP errors, because the outputs for other classes are effectively zero, so the explanation cannot properly distribute contributions.\n",
        "\n",
        "This happens because:\n",
        "\t•\tExtreme class imbalance → boot_delete dominates (~70% of sequences).\n",
        "\t•\tMinority classes too rare → the CNN never sees enough examples to learn patterns.\n",
        "\t•\tClass weights alone aren’t enough; we need oversampling or sequence-level augmentation."
      ],
      "metadata": {
        "id": "s7y6Uakht2DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import numpy as np\n",
        "\n",
        "# Flatten sequences for oversampling\n",
        "X_flat = X_sequences.reshape(len(X_sequences), -1)\n",
        "y_flat = y_sequences\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_flat, y_flat)\n",
        "\n",
        "# Reshape back into sequences\n",
        "X_resampled = X_resampled.reshape(-1, X_sequences.shape[1], X_sequences.shape[2])\n",
        "y_resampled_cat = to_categorical(y_resampled)\n",
        "\n",
        "print(\"Before oversampling:\", np.bincount(y_flat))\n",
        "print(\"After oversampling:\", np.bincount(y_resampled))"
      ],
      "metadata": {
        "id": "Va4KlYKbtyU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_resampled, y_resampled_cat,\n",
        "    validation_data=(X_test, y_test),  # keep test set unchanged\n",
        "    epochs=30,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "aiott54ptwHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Predictions shape:\", y_pred_classes.shape)\n",
        "print(\"True labels shape:\", y_true.shape)\n",
        "\n",
        "# Classification report\n",
        "target_names = [str(c) for c in label_encoder.classes_]\n",
        "report = classification_report(y_true, y_pred_classes, target_names=target_names, zero_division=0)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=target_names,\n",
        "            yticklabels=target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Per-class accuracy\n",
        "print(\"Per-class accuracy:\")\n",
        "for i, class_name in enumerate(target_names):\n",
        "    idx = y_true == i\n",
        "    if np.sum(idx) > 0:\n",
        "        class_acc = np.sum(y_pred_classes[idx] == i) / np.sum(idx)\n",
        "        print(f\"  {class_name}: {class_acc:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {class_name}: No samples in test set\")\n",
        "\n",
        "# Training history plots\n",
        "if 'history' in globals():\n",
        "    plt.figure(figsize=(12,4))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history.history['accuracy'], label='train')\n",
        "    plt.plot(history.history['val_accuracy'], label='val')\n",
        "    plt.title(\"Model Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='val')\n",
        "    plt.title(\"Model Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rBwQl29UtrQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild CNN from scratch\n",
        "input_shape = X_resampled.shape[1:]  # (sequence_length, num_features)\n",
        "\n",
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "x = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "outputs = tf.keras.layers.Dense(y_resampled_cat.shape[1], activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "cFo0dAGYtopZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_resampled = np.nan_to_num(X_resampled, nan=0.0, posinf=1e5, neginf=-1e5)"
      ],
      "metadata": {
        "id": "1YcHsdGatm_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Only scale numeric columns\n",
        "scaler = StandardScaler()\n",
        "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
        "\n",
        "# One-hot categorical columns are already 0/1 → no scaling needed"
      ],
      "metadata": {
        "id": "tpV-FgNKtlWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clip extreme values to [-10, 10] to prevent exploding activations\n",
        "X_resampled = np.clip(X_resampled, -10, 10)"
      ],
      "metadata": {
        "id": "JPrDif0stj7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=1e-4)  # lower LR for stability\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "OUk-spNOtihN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Any NaNs in y_resampled_cat?\", np.isnan(y_resampled_cat).any())\n",
        "print(\"Any Infs in y_resampled_cat?\", np.isinf(y_resampled_cat).any())\n",
        "print(\"Min/Max values in y_resampled_cat:\", y_resampled_cat.min(), y_resampled_cat.max())"
      ],
      "metadata": {
        "id": "GNOW81XWtg-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace any remaining NaNs and infinities\n",
        "X_resampled = np.nan_to_num(X_resampled, nan=0.0, posinf=1e5, neginf=-1e5)\n",
        "\n",
        "# Convert to float32\n",
        "X_resampled = X_resampled.astype(np.float32)\n",
        "y_resampled_cat = y_resampled_cat.astype(np.float32)\n",
        "\n",
        "# Clip extreme values for stability\n",
        "X_resampled = np.clip(X_resampled, -10, 10)"
      ],
      "metadata": {
        "id": "pDCWlbwwtfes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=1e-4)  # smaller LR prevents exploding gradients"
      ],
      "metadata": {
        "id": "LOBiu9-FteOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "inputs = tf.keras.Input(shape=X_resampled.shape[1:])\n",
        "x = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(inputs)\n",
        "x = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(y_resampled_cat.shape[1], activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "31nppNy_tcEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up validation/test set\n",
        "X_test = np.nan_to_num(X_test, nan=0.0, posinf=1e5, neginf=-1e5)\n",
        "X_test = X_test.astype(np.float32)\n",
        "\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "# Clip extreme values for stability\n",
        "X_test = np.clip(X_test, -10, 10)"
      ],
      "metadata": {
        "id": "Ql8GHf1MtaiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain briefly after cleaning X_test\n",
        "history = model.fit(\n",
        "    X_resampled, y_resampled_cat,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=5,            # short run, just to verify val_loss is no longer NaN\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "mCEITbD9tY4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "01i83yAUtXgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"cnn_model.h5\")\n",
        "# Later or in another notebook\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(\"cnn_model.h5\")"
      ],
      "metadata": {
        "id": "hOaxAvkVtTEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "“too good” an accuracy?\n",
        "\n",
        "\t1.\tTrain is heavily oversampled → the model sees tons of synthetic repetitions of minority classes. This makes it very easy to memorize class patterns.\n",
        "\t2.\tIf the features are strongly correlated with the labels (like program or pid giving away the operation), the model may have “shortcuts” that make predictions trivial.\n",
        "\t3.\tThe gap: Train ≈ 99.8% accuracy vs Test ≈ 99.6% — very small → possible leakage in preprocessing (scaler/ohe fit on all data).\n",
        "\n",
        "\n",
        "  Right now, RandomOverSampler is fully balancing all classes → can cause overfitting.\n",
        "\n",
        "Better strategies:\n",
        "\t•\timblearn.over_sampling.SMOTE (synthesizes new minority samples instead of duplicating).\n",
        "\t•\tLimit oversampling: instead of making classes perfectly balanced, bring them up to e.g. 50% of the majority class."
      ],
      "metadata": {
        "id": "Ot-ZIW8_tUvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Flatten to 2D for imputer\n",
        "X_train_2d = X_train.reshape(len(X_train), -1)\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X_train_imputed = imputer.fit_transform(X_train_2d)\n",
        "\n",
        "# Reshape back to sequences\n",
        "X_train = X_train_imputed.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])"
      ],
      "metadata": {
        "id": "Nby8nY4GtSqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Get class counts\n",
        "unique, counts = np.unique(y_train_int, return_counts=True)\n",
        "class_counts = dict(zip(unique, counts))\n",
        "print(\"Original class counts:\", class_counts)\n",
        "\n",
        "# Define sampling strategy (e.g. minority = 50% of majority)\n",
        "majority_class = max(class_counts, key=class_counts.get)\n",
        "majority_count = class_counts[majority_class]\n",
        "\n",
        "sampling_strategy = {\n",
        "    cls: int(0.5 * majority_count) if count < 0.5 * majority_count else count\n",
        "    for cls, count in class_counts.items()\n",
        "}\n",
        "\n",
        "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
        "\n",
        "X_train_res, y_train_res = smote.fit_resample(\n",
        "    X_train.reshape(len(X_train), -1), y_train_int\n",
        ")\n",
        "\n",
        "# Reshape back to sequences\n",
        "X_train_res = X_train_res.reshape(-1, X_train.shape[1], X_train.shape[2])\n",
        "y_train_res_cat = to_categorical(y_train_res)\n",
        "\n",
        "print(\"Resampled class counts:\", np.bincount(y_train_res))"
      ],
      "metadata": {
        "id": "n5OOzuMntP6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Baseline accuracy\n",
        "y_pred_base = model.predict(X_test)\n",
        "baseline_acc = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_base, axis=1))\n",
        "\n",
        "importances = []\n",
        "for i in range(X_test.shape[2]):  # loop over features\n",
        "    X_test_perm = X_test.copy()\n",
        "    np.random.shuffle(X_test_perm[:, :, i])  # permute feature\n",
        "    y_pred_perm = model.predict(X_test_perm)\n",
        "    acc_perm = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_perm, axis=1))\n",
        "    importances.append(baseline_acc - acc_perm)\n",
        "\n",
        "# Rank features by importance\n",
        "feature_names = numeric_features + list(ohe.get_feature_names_out(categorical_features))\n",
        "feat_importances = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Top 15 most important features:\")\n",
        "for name, score in feat_importances[:15]:\n",
        "    print(f\"{name}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "DZNciiNRtNFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations from feature importance\n",
        "\n",
        "\t•\tThe top features are mostly numeric system metrics:\n",
        "\t•\tmem_used_max, load15_mean, cpu_user_max, warning_logs, total_traces, missing_data, etc.\n",
        "\t•\tSome categorical identifiers appear (Hostname_wally123, pid_6.0) but their importance is much smaller.\n",
        "\t•\tMany features have near-zero contribution → keeping them might just add noise and risk overfitting.\n",
        "\n",
        "  What this tells us\n",
        "\n",
        "\t1.\tSystem metrics drive the model — makes sense, the operation affects CPU/memory/load.\n",
        "\t2.\tIdentifiers (hostname/pid) contribute, but minimally → including them is optional.\n",
        "\t•\tKeeping them might help a little, but can also cause overfitting or memorization of host-specific patterns.\n",
        "\t3.\tTop 10–15 features explain most of the predictive power.\n",
        "\t•\tYou can safely drop the rest to simplify the model and improve generalization."
      ],
      "metadata": {
        "id": "wROU2n4ftMTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_features = [name for name, _ in feat_importances[:15]]\n",
        "\n",
        "# For numeric features\n",
        "X_train_top = X_train_res[:, :, [numeric_features.index(f) for f in top_features if f in numeric_features]]\n",
        "\n",
        "# For categorical features\n",
        "cat_indices = [i for i, f in enumerate(feature_names) if f in top_features and f not in numeric_features]\n",
        "# If using OHE, you can slice columns accordingly\n",
        "# X_train_top_cat = X_train_res[:, :, cat_indices]"
      ],
      "metadata": {
        "id": "GXCXA5c0tJpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Select top features\n",
        "top_features = [name for name, _ in feat_importances[:15]]\n",
        "\n",
        "# Get indices for numeric features\n",
        "numeric_idx = [numeric_features.index(f) for f in top_features if f in numeric_features]\n",
        "\n",
        "# Get indices for categorical features (OHE columns)\n",
        "cat_idx = [i for i, f in enumerate(ohe.get_feature_names_out(categorical_features)) if f in top_features]\n",
        "\n",
        "# Combine indices\n",
        "top_indices = numeric_idx + [len(numeric_features) + i for i in cat_idx]\n",
        "\n",
        "# Extract top features\n",
        "X_train_top = X_train[:, :, top_indices]\n",
        "X_test_top = X_test[:, :, top_indices]\n",
        "\n",
        "# Impute any remaining NaNs\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X_train_top = imputer.fit_transform(X_train_top.reshape(len(X_train_top), -1))\n",
        "X_test_top = imputer.transform(X_test_top.reshape(len(X_test_top), -1))\n",
        "\n",
        "# Reshape back to sequences\n",
        "seq_len = X_train.shape[1]\n",
        "X_train_top = X_train_top.reshape(-1, seq_len, len(top_indices))\n",
        "X_test_top = X_test_top.reshape(-1, seq_len, len(top_indices))\n",
        "\n",
        "# Step 3: Re-run SMOTE on training data\n",
        "y_train_int = np.argmax(y_train, axis=1)  # ensure integer labels\n",
        "\n",
        "# Define partial oversampling strategy (50% of majority)\n",
        "unique, counts = np.unique(y_train_int, return_counts=True)\n",
        "class_counts = dict(zip(unique, counts))\n",
        "majority_count = max(class_counts.values())\n",
        "sampling_strategy = {cls: int(0.5*majority_count) if count < 0.5*majority_count else count\n",
        "                     for cls, count in class_counts.items()}\n",
        "\n",
        "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_top.reshape(len(X_train_top), -1), y_train_int)\n",
        "\n",
        "# Reshape back to sequences\n",
        "X_train_res = X_train_res.reshape(-1, seq_len, len(top_indices))\n",
        "y_train_res_cat = to_categorical(y_train_res)\n",
        "\n",
        "print(\"Resampled class counts:\", np.bincount(y_train_res))\n",
        "\n",
        "# Build smaller CNN\n",
        "input_shape = (seq_len, len(top_indices))\n",
        "inputs = Input(shape=input_shape)\n",
        "x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv1D(16, kernel_size=3, activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(y_train_res_cat.shape[1], activation='softmax')(x)\n",
        "\n",
        "model_small = Model(inputs, outputs)\n",
        "model_small.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model_small.fit(\n",
        "    X_train_res, y_train_res_cat,\n",
        "    validation_data=(X_test_top, y_test),\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_pred = np.argmax(model_small.predict(X_test_top), axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "id": "tKM6z12etFIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOtiFQJ_tDwF"
      },
      "outputs": [],
      "source": [
        "model_small.save(\"cnn_top15_features.h5\")\n",
        "# Save preprocessing objects\n",
        "import joblib\n",
        "joblib.dump(imputer, \"imputer.pkl\")\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "joblib.dump(ohe, \"ohe.pkl\")\n",
        "joblib.dump(label_encoder, \"label_encoder.pkl\")"
      ]
    }
  ]
}