---
title: "Log_level classification"
output: html_notebook
---

```{r}
# Load required libraries
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)
library(readr)
library(lubridate)
library(ggplot2)
library(GGally)
library(corrplot)
library(scales)
library(tidyverse)
```

# Load the data in correctly

```{r}
# Define base path (adjust if needed)
base_path <- "~/Desktop/DXC AI team assesment/data/processed/sequential_data"

# Load datasets
logs <- read_csv(file.path(base_path, "logs", "logs_aggregated_sequential.csv"))

# Check structure
glimpse(logs)
```

```{r}
# Correct atonmic data types
logs <- logs %>%
  mutate(
    # convert categorical columns to factors
    across(c(user_id, project_domain, log_level, tenant_id, programname, 
             python_module, Logger, user_domain, http_method, `_type`, `_index`),
           as.factor),
    
    # parse "Timestamp" with milliseconds
    Timestamp = parse_date_time(Timestamp, orders = "ymd HMS", truncated = 3),
    
    # ensure @timestamp is POSIXct
    `@timestamp` = as.POSIXct(`@timestamp`, tz = "UTC")
  )

# Make sure the changes were made
str(logs)
```

# Handle missingness

```{r}
# Check for missingness
colSums(is.na(logs))
```
I don’t need to impute the numeric columns in this dataset, because the missingness is meaningful:

* Pid missing → log didn’t come from a process where a PID is relevant.
* http_status, http_method, http_version, http_url missing → log is not an HTTP request.

Trying to impute numbers like Pid = 0 or http_status = 200 would misrepresent the data, because those rows are simply non-HTTP/system logs.

User / project context (41,549 NAs each)
* user_id, project_domain, tenant_id, request_id, user_domain, domain_id

Two types of log records usually exist:
* Application-level logs → user_id, project_domain, tenant_id
* System/service-level logs → generated by the platform itself, not directly related to any user or project. These logs often don’t have a user_id, tenant_id, or domain_id because they aren’t associated with a particular customer/user action.

```{r}
# Are the missing numerical rows overlapping with missing ID columns? (PID)

# Count rows where both user_id and Pid are NA
logs %>%
  filter(is.na(user_id) & is.na(Pid)) %>%
  nrow() #  These are system-level logs that also don’t have a process ID & matches the missing log_level count

# Mark these rows as system-level logs by creating a new column
logs <- logs %>%
  mutate(
    log_type = case_when(
      is.na(user_id) & is.na(Pid) ~ "system",
      TRUE ~ "user"
    )
  )

# Check the number of system vs user rows
logs %>%
  count(log_type)

# Check the number of rows where log_level is NA among system-level logs
logs %>%
  filter(log_type == "system") %>%
  summarise(na_log_level = sum(is.na(log_level))) # System rows have no log_level's - these add noise - especially since == ERROR is our target. Drop these rows.

# Drop system-level rows entirely for your log-level-focused analysis
logs_clean <- logs %>%
  filter(log_type != "system")
```

```{r}
# Are the missing numerical rows overlapping with missing ID columns? (HTTP)

# Count rows where user_id NA & http_status NA
logs_clean %>%
  filter(is.na(user_id) & is.na(http_status)) %>%
  nrow()

# Preview just the first few columns and rows
logs_clean %>%
  filter(is.na(user_id) & is.na(http_status)) %>%
  select(user_id, project_domain, tenant_id, request_id, user_domain, domain_id,
         log_level, Pid, python_module, Logger, http_status, http_method, http_version, http_url) %>%
  head(20) # They contain valid log_level information (INFO/WARNING) that could be relevant for analysis & HTTP-related NAs are fine — they indicate non-HTTP events. Dropping them would remove actual events, potentially biasing counts of log levels or trends over time

# Create a third category like "background" to differentiate them from normal user-triggered events
logs_clean <- logs_clean %>%
  mutate(
    log_type = case_when(
      is.na(user_id) & !is.na(Pid) ~ "background", # log_type = "background" represents log events that Have no user or project context i.e. user_id is NA, Are associated with a running process i.e. Pid is not NA or python_module is present, Contain a valid log level, & Are typically non-HTTP
      TRUE ~ "user"
    )
  )

# Check the unique log_level values among the background logs
logs_clean %>%
  filter(log_type == "background") %>%
  count(log_level) # keeping the background rows is reasonable because they contain valid log events, including some ERRORs, which could be important for system monitoring or anomaly detection

```

```{r}
# Check the number of each level in log_level
summary(logs_clean$log_level)

# Normalise log_level case
logs_clean <- logs_clean %>%
  mutate(log_level = toupper(as.character(log_level))) %>%
  mutate(log_level = factor(log_level, levels = c("INFO", "WARNING", "ERROR")))
```

```{r}
# Compare Timestamp columns
head(logs_clean %>% select(Timestamp, `@timestamp`))

# @timestamp has no missing values, but it’s exactly 1 hour behind Timestamp in the rows where both exist (e.g. 17:00:48 vs 16:00:48)

# The true experiment interval is based on TRACE TIME (UTC)

# Check the timezone attribute
attr(logs_clean$`@timestamp`, "tzone")
attr(logs_clean$`Timestamp`, "tzone") # By default, R assigns "UTC" unless you explicitly set tz

# Timestamp = @timestamp + 1 hour therefore we assume @timestamp is UTC, and Timestamp is local time (+1), therefore we must drop Timestamp
logs_clean <- logs_clean %>%
  select(-Timestamp)
```

```{r}
# Check the number of none-NA rows in http related columns
http_cols <- c("http_status", "http_method", "http_version", "http_url")
sapply(logs_clean[http_cols], function(x) sum(!is.na(x))) # There are no none-NA rows in HTTP related columns

# Drop http-related columns from the clean dataset
logs_clean <- logs_clean %>%
  select(-http_status, -http_method, -http_version, -http_url) # IMPORTANT: this dataset now assumes every row is a non-HTTP event
```

```{r}
# Amend column names for ease of understanding
logs_clean <- logs_clean %>%
  rename(log_date = `_index`) %>%
  select(-`_type`) # Assume the data's type is fluentd as there is only one level present in all rows
```


# Check levels are correct

```{r}
# project_domain	"-", "default"
# user_domain	"-", "default"
# tenant_id	"-", GUID-like strings
# user_id	"-" # background/system columns consistently use "-" or "default" identifiers

# Define background logs & assign log_type for these rows
logs_clean <- logs_clean %>%
  mutate(
    log_type = ifelse(
      project_domain == "-" & user_domain == "-" & tenant_id == "-",
      "background",
      log_type
    )
  )

# for all rows where log_type == "background", replace the user/project/tenant columns with a consistent identifier
logs_clean <- logs_clean %>%
  mutate(
    request_id     = ifelse(log_type == "background", "None_User_Service", as.character(request_id)),
    user_id        = ifelse(log_type == "background", 0, as.numeric(user_id)),
    project_domain = ifelse(log_type == "background", 0, as.numeric(project_domain)),
    user_domain    = ifelse(log_type == "background", 0, as.numeric(user_domain)),
    tenant_id      = ifelse(log_type == "background", 0, as.numeric(tenant_id))) %>%
  mutate(
    request_id = as.factor(request_id)
  )

# domain_id only has one meaningful value ("-") for the vast majority of rows and NA for the background rows. It carries no variability, so it provides no useful signal for analysis. Keeping it could confuse models or add unnecessary columns.
logs_clean <- logs_clean %>%
  select(-domain_id) # Drop redundant columns
```

# For the remaining ~4.2k NA's impute using a funnel approach, starting from the most general

```{r}
# Helper function to impute most common value in a vector
impute_most_common <- function(x, fallback = NA) {
  if(all(is.na(x))) {
    return(rep(fallback, length(x)))
  } else {
    most <- names(sort(table(x), decreasing = TRUE))[1]
    x[is.na(x)] <- most
    return(x)
  }
}

# Global fallback values
fallback_request <- "None_User_Service"
fallback_numeric <- 0

# Impute log_type for suspected background rows
logs_clean <- logs_clean %>%
  mutate(
    log_type = case_when(
      log_type == "background" ~ "background",
      is.na(log_type) & is.na(user_id) & !is.na(Pid) ~ "background",
      TRUE ~ as.character(log_type)
    )
  )

# Hierarchical imputation of columns
# project_domain -> user_domain -> request_id -> user_id
# numeric columns get 0; categorical columns get fallback string

# project_domain (numeric)
logs_clean <- logs_clean %>%
  group_by(tenant_id) %>%
  mutate(
    project_domain = as.numeric(impute_most_common(as.numeric(project_domain), fallback = fallback_numeric))
  ) %>%
  ungroup()


# user_domain (numeric)
logs_clean <- logs_clean %>%
  group_by(project_domain) %>%
  mutate(user_domain = impute_most_common(user_domain, fallback_numeric)) %>%
  ungroup()

# request_id (factor / categorical)
logs_clean <- logs_clean %>%
  group_by(user_domain) %>%
  mutate(request_id = impute_most_common(as.character(request_id), fallback_request)) %>%
  ungroup() %>%
  mutate(request_id = as.factor(request_id))

# user_id (numeric)
logs_clean <- logs_clean %>%
  group_by(request_id) %>%
  mutate(user_id = impute_most_common(user_id, fallback_numeric)) %>%
  ungroup()

# Ensure numeric/factor consistency
logs_clean <- logs_clean %>%
  mutate(
    user_id = as.numeric(user_id),
    project_domain = as.numeric(project_domain),
    user_domain = as.numeric(user_domain),
    tenant_id = as.numeric(tenant_id),
    log_type = as.factor(log_type)
  )

logs_clean <- logs_clean %>%
  group_by(project_domain) %>%
  mutate(tenant_id = impute_most_common(tenant_id, fallback = 0)) %>%
  ungroup()
```

```{r}
# Impute the most commmon level in Looger and Python_module
logs_clean <- logs_clean %>%
  mutate(
    Logger = as.character(Logger),
    python_module = as.character(python_module),
    Logger = ifelse(is.na(Logger), "openstack.nova", Logger),
    python_module = ifelse(is.na(python_module), "eventlet.wsgi.server", python_module)
  )

# Check the number of NAs per column
colSums(is.na(logs_clean))
```

There is no longer missingness in the data, level inaccuracies have been addressed, redundant fetures have been dropped. This data is ready for exploratory data analysis.

# ------------------------------------------------------------------------------

# Exploratory data analysis

```{r}
# Print summary statistics
summary(logs_clean)
str(logs_clean)
```

```{r}
numeric_cols <- logs_clean %>% select_if(is.numeric)
numeric_summary <- numeric_cols %>% summary()
print(numeric_summary)

# Histograms for numeric variables
numeric_cols %>%
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(fill = "steelblue", bins = 30, alpha = 0.8) +
  facet_wrap(~variable, scales = "free") +
  theme_minimal() +
  labs(title = "Distribution of Numeric Variables")
```

```{r}
categorical_cols <- logs_clean %>% select_if(~is.factor(.) | is.character(.))
categorical_cols %>%
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = fct_lump(value, 10))) +
  geom_bar(fill = "coral") +
  facet_wrap(~variable, scales = "free") +
  theme_minimal() +
  labs(title = "Top 10 Categories per Categorical Variable", x = "Category", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
logs_clean %>%
  group_by(log_level) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = log_level, y = count, fill = log_level)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("INFO" = "green", "WARNING" = "orange", "ERROR" = "red")) +
  theme_minimal() +
  labs(title = "Count of Logs by Log Level", y = "Number of Logs", x = "Log Level")
```

```{r}
logs_clean %>%
  mutate(date = as_date(`@timestamp`)) %>%
  group_by(date, log_level) %>%
  summarise(count = n(), .groups = "drop") %>%
  ggplot(aes(x = date, y = count, color = log_level)) +
  geom_line(sise = 1) +
  geom_point(sise = 1) +
  theme_minimal() +
  labs(title = "Logs Over Time by Log Level", x = "Date", y = "Count", color = "Log Level") +
  scale_x_date(labels = date_format("%Y-%m-%d"))
```

```{r}
logs_clean %>%
  group_by(programname, log_level) %>%
  summarise(count = n(), .groups = "drop") %>%
  ggplot(aes(x = fct_reorder(programname, count), y = count, fill = log_level)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Logs per Program by Log Level", x = "Program", y = "Number of Logs", fill = "Log Level")
```

```{r}
logs_clean %>%
  group_by(user_id) %>%
  summarise(logs_count = n(), .groups = "drop") %>%
  ggplot(aes(x = logs_count)) +
  geom_histogram(fill = "purple", bins = 30) +
  theme_minimal() +
  labs(title = "Distribution of Logs per User", x = "Number of Logs", y = "Count of Users")
```

```{r}
numeric_corr <- cor(numeric_cols, use = "pairwise.complete.obs")
corrplot(numeric_corr, method = "color", type = "upper", tl.cex = 0.8, addCoef.col = "black", number.cex = 0.7)
```

```{r}
selected_numeric <- logs_clean %>% select(user_id, Pid, project_domain, user_domain)
ggpairs(selected_numeric, title = "Pairwise Relationships for Selected Numeric Variables")
```

```{r}
logs_clean %>%
  mutate(hour = hour(`@timestamp`)) %>%
  group_by(hour) %>%
  summarise(count = n(), .groups = "drop") %>%
  ggplot(aes(x = hour, y = count)) +
  geom_line(color = "darkblue", sise = 1) +
  geom_point(color = "red") +
  theme_minimal() +
  labs(title = "Logs by Hour of Day", x = "Hour", y = "Number of Logs")
```

# ------------------------------------------------------------------------------

# What causes an error?

```{r}
# Count logs by level
logs_clean %>%
  group_by(log_level) %>%
  summarise(count = n())

```

```{r}
# Create a simplified log_level variable (ERROR vs Non-ERROR)
logs_clean <- logs_clean %>%
  mutate(error_flag = ifelse(log_level == "ERROR", 1, 0))

# Programname vs errors
logs_clean %>%
  group_by(programname) %>%
  summarise(total_logs = n(),
            error_logs = sum(error_flag),
            error_rate = error_logs / total_logs) %>%
  arrange(desc(error_rate)) %>%
  ggplot(aes(x = fct_reorder(programname, error_rate), y = error_rate)) +
  geom_col(fill = "red") +
  coord_flip() +
  labs(title = "Error Rate by Program", y = "Error Rate", x = "Program") +
  theme_minimal()

```

```{r}
# User-related error rate
logs_clean %>%
  group_by(user_id) %>%
  summarise(total_logs = n(),
            error_logs = sum(error_flag),
            error_rate = error_logs / total_logs) %>%
  filter(error_logs > 0) %>%
  arrange(desc(error_rate)) %>%
  head(10)  # top 10 users with highest error rate

# Host-related error rate
logs_clean %>%
  group_by(Hostname) %>%
  summarise(total_logs = n(),
            error_logs = sum(error_flag),
            error_rate = error_logs / total_logs) %>%
  filter(error_logs > 0) %>%
  arrange(desc(error_rate))

```

User-based error rates

Insights:
* user_id = 30 has very few logs (21) but 3 errors → high error rate (~14%).
* Most other users with many logs have extremely low error rates (<0.3%).
* Observation: Small sample sises can inflate error rate, so it’s worth investigating whether these few errors are coincidental or systemic.

Host-based error rates

Insights:
* wally123 has the highest error rate (~0.8%) despite not being the host with the most logs.
* wally113 handles the majority of logs but has an extremely low error rate.
* Observation: Certain hosts might be more prone to errors, independent of volume.

Key Takeaways

* Error rates are more meaningful than raw counts because of severe imbalance.
* Users or hosts with low total logs but multiple errors can disproportionately impact the error rate.
* Programname and Pid likely also influence error occurrence—these should be analyzed similarly.
* Extreme imbalance (28 errors out of 102k logs) means any statistical model must weight errors heavily to detect patterns.

# -------------------------------------------------------------------

# Calculate error rates per Programname and per Pid and User

```{r}
# Ensure PID is treated as factor
logs_clean <- logs_clean %>%
  mutate(Pid = as.factor(Pid),
         error_flag = ifelse(log_level == "ERROR", 1, 0))
```

```{r}
program_errors <- logs_clean %>%
  group_by(programname) %>%
  summarise(total_logs = n(),
            error_logs = sum(error_flag),
            error_rate = error_logs / total_logs,
            .groups = "drop") %>%
  arrange(desc(error_rate))

# Display top 10 programs with highest error rates
head(program_errors, 10)
```

```{r}
pid_errors <- logs_clean %>%
  group_by(Pid) %>%
  summarise(total_logs = n(),
            error_logs = sum(error_flag),
            error_rate = error_logs / total_logs,
            .groups = "drop") %>%
  arrange(desc(error_rate))

# Display top 10 PIDs with highest error rates
head(pid_errors, 10)
```

```{r}
# Visualise error rate per PID (top 20 only for clarity)
top_pid_errors <- pid_errors %>% slice_max(error_rate, n = 20)

ggplot(top_pid_errors, aes(x = fct_reorder(Pid, error_rate), y = error_rate)) +
  geom_col(fill = "darkred") +
  coord_flip() +
  labs(title = "Top 20 PIDs by Error Rate",
       x = "PID",
       y = "Error Rate") +
  theme_minimal()
```

```{r}
program_pid_errors <- logs_clean %>%
  group_by(programname, Pid) %>%
  summarise(total_logs = n(),
            error_logs = sum(error_flag),
            error_rate = error_logs / total_logs,
            .groups = "drop") %>%
  filter(error_logs > 0)

ggplot(program_pid_errors, aes(x = Pid, y = programname, fill = error_rate)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "red") +
  labs(title = "Error Rate by Programname and PID",
       x = "PID",
       y = "Programname",
       fill = "Error Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Insights:

* Small-sample PIDs (2796, 15117, 18133, 19361)
*   Each of these PIDs has only 2 logs, and both are errors → 100% error rate.
*   This is misleading because the sample sise is tiny; these are probably short-lived processes.

* Larger-sample PIDs (6, 7, 20, 21)
*   These have thousands of logs and a few errors.
*   Their true error rate is extremely low (~0.01–0.05%).
*   These are more representative of the system behavior.

* Zero-error PIDs (13, 14)
*   Even though they have >100 logs, they produced no errors, suggesting these processes are more stable.

Key Takeaways

PID alone is not sufficient to identify “causes” of errors because short-lived processes with tiny logs can show misleadingly high error rates.

To find meaningful patterns, we should focus on:

*   Programname + Pid combinations (to see which programs’ processes are prone to errors).
*   Host + Programname + Pid (to account for machine-specific issues).
*   Possibly user_id if user-driven errors exist.


```{r}
program_pid_errors <- logs_clean %>%
  group_by(programname, Pid) %>%
  summarise(
    total_logs = n(),
    error_logs = sum(error_flag),
    error_rate = error_logs / total_logs,
    .groups = "drop"
  ) %>%
  filter(error_logs > 0)  # focus only on combinations with errors

# Display top combinations by error_rate (exclude tiny logs if desired)
program_pid_errors %>%
  arrange(desc(error_rate)) %>%
  head(20)

# Visualise top Programname-Pid combinations
ggplot(program_pid_errors %>% slice_max(error_rate, n = 20),
       aes(x = fct_reorder(Pid, error_rate), y = error_rate, fill = programname)) +
  geom_col() +
  coord_flip() +
  labs(title = "Top Programname + Pid Combinations by Error Rate",
       x = "Pid", y = "Error Rate", fill = "Programname") +
  theme_minimal()
```

```{r}
host_program_pid_errors <- logs_clean %>%
  group_by(Hostname, programname, Pid) %>%
  summarise(
    total_logs = n(),
    error_logs = sum(error_flag),
    error_rate = error_logs / total_logs,
    .groups = "drop"
  ) %>%
  filter(error_logs > 0)  # focus on combinations with errors

# Visualise as heatmap
ggplot(host_program_pid_errors %>% slice_max(error_rate, n = 50),
       aes(x = Pid, y = programname, fill = error_rate)) +
  geom_tile() +
  facet_wrap(~Hostname, scales = "free_x") +
  scale_fill_gradient(low = "white", high = "red") +
  labs(title = "Host + Programname + Pid Error Rates",
       x = "Pid", y = "Programname", fill = "Error Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

```{r}
user_program_pid_errors <- logs_clean %>%
  group_by(user_id, programname, Pid) %>%
  summarise(
    total_logs = n(),
    error_logs = sum(error_flag),
    error_rate = error_logs / total_logs,
    .groups = "drop"
  ) %>%
  filter(error_logs > 0)

# Top users by error rate
user_program_pid_errors %>%
  arrange(desc(error_rate)) %>%
  head(20)

# Visualise top 20 user-program-Pid combinations
ggplot(user_program_pid_errors %>% slice_max(error_rate, n = 20),
       aes(x = fct_reorder(Pid, error_rate), y = error_rate, fill = programname)) +
  geom_col() +
  facet_wrap(~user_id, scales = "free_y") +
  coord_flip() +
  labs(title = "Top User + Programname + Pid Combinations by Error Rate",
       x = "Pid", y = "Error Rate", fill = "Programname") +
  theme_minimal()
```
Observations:

* User 0 dominates logs with low error rates, across multiple programs and PIDs.
* User 30 has only 10 logs for glance-api Pid 21 but already 3 errors → high error rate, likely due to small sample.
* User 3 shows moderate activity with nova-conductor and nova-compute, small sample sises but a few errors.
* Most errors occur in processes with small numbers of logs → highlights that small samples can produce misleadingly high error rates.


Key Takeaways:

* Tiny sample sises dominate top error rates → the extreme error percentages (1.0, 0.5, 0.3) are mostly due to very few logs.

* Processes with thousands of logs (neutron-openvswitch-agent, glance-api, nova-compute) have very low error rates, representing the system’s “true” error-prone behavior.

*User-driven errors are minimal; only a few users (like 30 and 3) have noticeable error activity, but small sample sises exaggerate the rate.

Since I only have 28 errors in >102k logs (~0.03%), any attempt to “train a model” (e.g., classification) will:

* Be overwhelmed by the class imbalance.
* Get distorted by tiny sample sises (libvirt with 2 logs, etc.).
* Produce noise rather than insight.

So instead of trying to predict errors, you can ask other questions about system behavior.

# ------------------------------------------------------------------------------

# What do these logs tell me about how the system behaves, where it’s busy, and where performance issues or anomalies might be hiding?

```{r}
# Logs per program
logs_clean %>%
  count(programname, sort = TRUE)

# Logs per host
logs_clean %>%
  count(Hostname, sort = TRUE)

# Logs per program + host
logs_clean %>%
  count(Hostname, programname, sort = TRUE)

# Logs per user
logs_clean %>%
  count(user_id, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(x = reorder(user_id, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 Users by Log Volume", x = "User ID", y = "Log Count")

```

Insights:
* The system is dominated by neutron-server, nova-api, nova-compute, and glance-api.
* Together, these 4 account for ~94% of logs → the “core services.”
* Others like neutron-dhcp-agent and keystone contribute tiny volumes — less critical in terms of log noise.

* wally113 is doing the heavy lifting (80% of all logs).
* Other hosts (wally122, wally124, wally117) handle smaller chunks of work.
* wally123 is almost idle by comparison (608 logs only).

This suggests either:
* wally113 is a central node (controller/API-heavy).
* Others are compute/storage nodes.
* Potential imbalance → maybe a capacity or redundancy risk.


wally113 hosts the controller plane:
* neutron-server, nova-api, glance-api, nova-scheduler.
* These are central OpenStack services, consistent with a controller node.
* wally122, wally124, wally117 run nova-compute → these are worker/compute nodes.
* neutron-openvswitch-agent appears across multiple hosts → network dataplane spread across compute nodes.
* wally123 isn’t in this top list → could be a standby/utility node.

# ------------------------------------------------------------------------------

# Introduce Metrics data

```{r}
metrics <- read.csv("~/Desktop/DXC AI team assesment/data/processed/sequential_data/metrics/combined_sequential_metrics.csv")
```

```{r}
# define true experiment window
start_time <- as.POSIXct("2019-11-19 17:38:39", tz = "UTC")
end_time   <- as.POSIXct("2019-11-20 01:30:00", tz = "UTC")

metrics_clean <- metrics %>%
  mutate(
    now = as.POSIXct(now, format = "%Y-%m-%d %H:%M:%S", tz = "UTC"),
    `@timestamp` = now - hours(1)   # shift back 1 hour
  ) %>%
  filter(`@timestamp` >= start_time & `@timestamp` <= end_time)

logs_clean <- logs_clean %>%
  filter(`@timestamp` >= start_time & `@timestamp` <= end_time)
```

```{r}
library(dplyr)

metrics_freq <- metrics_clean %>%
  arrange(`@timestamp`) %>%
  mutate(diff_sec = as.numeric(difftime(`@timestamp`, lag(`@timestamp`), units = "secs"))) %>%
  summarise(
    min_interval = min(diff_sec, na.rm = TRUE),
    median_interval = median(diff_sec, na.rm = TRUE),
    max_interval = max(diff_sec, na.rm = TRUE)
  )
```

```{r}
logs_freq <- logs_clean %>%
  arrange(`@timestamp`) %>%
  mutate(diff_sec = as.numeric(difftime(`@timestamp`, lag(`@timestamp`), units = "secs"))) %>%
  summarise(
    min_interval = min(diff_sec, na.rm = TRUE),
    median_interval = median(diff_sec, na.rm = TRUE),
    max_interval = max(diff_sec, na.rm = TRUE)
  )
```

logs data is events driven and there can be upto a minute where there is no data, metrics has data every second and multiple records in the same second. To make these datasets comparable, aggregate them into bins of 1 minute and make sure logs are aggregated into the same bin (time window)

```{r}
bin_sise <- "60 seconds"
```

```{r}
# --- Aggregate logs: keep counts + metadata ---
logs_agg <- logs_clean %>%
  mutate(bin_time = floor_date(`@timestamp`, bin_sise)) %>%
  group_by(Hostname, bin_time) %>%
  summarise(
    total_logs   = n(),
    error_logs   = sum(log_level == "ERROR", na.rm=TRUE),
    warning_logs = sum(log_level == "WARNING", na.rm=TRUE),
    info_logs    = sum(log_level == "INFO", na.rm=TRUE),
    user_ids     = paste(unique(user_id), collapse = ","),
    programs     = paste(unique(programname), collapse = ","),
    pids         = paste(unique(Pid), collapse = ","),
    .groups = "drop"
  )

# --- Aggregate metrics: keep full set ---
metrics_agg <- metrics_clean %>%
  mutate(bin_time = floor_date(`@timestamp`, bin_sise)) %>%
  group_by(node, bin_time) %>%
  summarise(
    cpu_user_mean = mean(cpu.user, na.rm=TRUE),
    cpu_user_max  = max(cpu.user, na.rm=TRUE),
    mem_used_mean = mean(mem.used, na.rm=TRUE),
    mem_used_max  = max(mem.used, na.rm=TRUE),
    load1_mean    = mean(load.min1, na.rm=TRUE),
    load5_mean    = mean(load.min5, na.rm=TRUE),
    load15_mean   = mean(load.min15, na.rm=TRUE),
    .groups = "drop"
  )

metrics_agg <- metrics_agg %>%
  rename(Hostname = node) %>%              # rename column
  mutate(Hostname = str_remove(Hostname, "_metrics"))  # clean values


# --- Merge logs + metrics ---
combined <- logs_agg %>%
  full_join(metrics_agg, by = c("Hostname", "bin_time"))
```


Now that metrics and logs data is aggregated by the same timeframe (60s bins), the datasets have merged. Investigate the relationships between metrics and logs.

# ------------------------------------------------------------------------------

```{r}
# select only numeric metrics + error_logs
corr_data <- combined %>%
  select(error_logs, cpu_user_mean, cpu_user_max,
         mem_used_mean, mem_used_max,
         load1_mean, load5_mean, load15_mean)

# correlation matrix
cor_matrix <- cor(corr_data, use = "complete.obs")
print(cor_matrix)

# visualise correlations
ggcorr(corr_data, label = TRUE, label_round = 2) +
  ggtitle("Correlation between errors and system metrics")

```

```{r}
# summarise errors and metrics by Hostname
host_summary <- combined %>%
  group_by(Hostname) %>%
  summarise(total_errors = sum(error_logs, na.rm = TRUE),
            avg_cpu = mean(cpu_user_mean, na.rm = TRUE),
            avg_mem = mean(mem_used_mean, na.rm = TRUE),
            .groups = "drop")

print(host_summary)

# bar plot errors by host
ggplot(host_summary, aes(x = reorder(Hostname, -total_errors), y = total_errors)) +
  geom_col(fill = "tomato") +
  coord_flip() +
  labs(title = "Total Errors per Host", x = "Hostname", y = "Error Count")

# CPU vs Memory per host
ggplot(host_summary, aes(x = avg_cpu, y = avg_mem, sise = total_errors, color = Hostname)) +
  geom_point(alpha = 0.7) +
  labs(title = "Host-level Behavior: CPU vs Memory",
       x = "Avg CPU Usage (%)", y = "Avg Memory Used (bytes)")

```

```{r}
# errors vs cpu over time (example host)
ggplot(combined %>% filter(Hostname == "wally113"),
       aes(x = bin_time)) +
  geom_line(aes(y = cpu_user_mean), color = "blue") +
  geom_line(aes(y = error_logs * 10), color = "red") + # scale errors for visibility
  labs(title = "CPU Usage vs Errors over Time (wally113)",
       y = "CPU (blue) / Errors*10 (red)", x = "Time")

# errors vs cpu over time (example host)
ggplot(combined %>% filter(Hostname == "wally123"),
       aes(x = bin_time)) +
  geom_line(aes(y = cpu_user_mean), color = "blue") +
  geom_line(aes(y = error_logs * 10), color = "red") + # scale errors for visibility
  labs(title = "CPU Usage vs Errors over Time (wally123)",
       y = "CPU (blue) / Errors*10 (red)", x = "Time")

# errors vs cpu over time (example host)
ggplot(combined %>% filter(Hostname == "wally124"),
       aes(x = bin_time)) +
  geom_line(aes(y = cpu_user_mean), color = "blue") +
  geom_line(aes(y = error_logs * 10), color = "red") + # scale errors for visibility
  labs(title = "CPU Usage vs Errors over Time (wally124)",
       y = "CPU (blue) / Errors*10 (red)", x = "Time")

# errors vs cpu over time (example host)
ggplot(combined %>% filter(Hostname == "wally122"),
       aes(x = bin_time)) +
  geom_line(aes(y = cpu_user_mean), color = "blue") +
  geom_line(aes(y = error_logs * 10), color = "red") + # scale errors for visibility
  labs(title = "CPU Usage vs Errors over Time (wally122)",
       y = "CPU (blue) / Errors*10 (red)", x = "Time")

# errors vs cpu over time (example host)
ggplot(combined %>% filter(Hostname == "wally117"),
       aes(x = bin_time)) +
  geom_line(aes(y = cpu_user_mean), color = "blue") +
  geom_line(aes(y = error_logs * 10), color = "red") + # scale errors for visibility
  labs(title = "CPU Usage vs Errors over Time (wally117)",
       y = "CPU (blue) / Errors*10 (red)", x = "Time")

# errors vs memory usage over time (all hosts)
ggplot(combined, aes(x = bin_time, y = mem_used_mean, color = error_logs > 0)) +
  geom_point(alpha = 0.6) +
  labs(title = "Memory Usage with Error Occurrence Highlighted",
       y = "Memory Used (bytes)", color = "Error > 0")
```

when CPU usage spikes in wally 113 errors spike. 



```{r}
# expand user_ids into long format
user_errors <- combined %>%
  filter(!is.na(user_ids)) %>%
  mutate(user_ids = strsplit(user_ids, ",")) %>%
  unnest(user_ids) %>%
  group_by(user_ids) %>%
  summarise(total_errors = sum(error_logs, na.rm = TRUE),
            total_logs = sum(total_logs, na.rm = TRUE),
            .groups = "drop") %>%
  mutate(error_rate = total_errors / total_logs)

# top users by error count
top_users <- user_errors %>%
  arrange(desc(total_errors)) %>%
  head(20)

print(top_users)

ggplot(top_users, aes(x = reorder(user_ids, -total_errors), y = total_errors)) +
  geom_col(fill = "purple") +
  coord_flip() +
  labs(title = "Top Users by Error Count", x = "User ID", y = "Error Count")
```


```{r}
# 1. Expand programs into long format
program_errors <- combined %>%
  filter(!is.na(programs)) %>%
  mutate(programs = strsplit(programs, ",")) %>%
  unnest(programs) %>%
  group_by(Hostname, programs) %>%
  summarise(total_errors = sum(error_logs, na.rm = TRUE),
            total_logs = sum(total_logs, na.rm = TRUE),
            avg_cpu = mean(cpu_user_mean, na.rm = TRUE),
            avg_mem = mean(mem_used_mean, na.rm = TRUE),
            .groups = "drop") %>%
  mutate(error_rate = total_errors / total_logs)

# 2. Look at top programs by errors
top_programs <- program_errors %>%
  arrange(desc(total_errors)) %>%
  head(20)

print(top_programs)

# 3. Visualise errors per program across hosts
ggplot(top_programs, aes(x = reorder(programs, -total_errors), y = total_errors, fill = Hostname)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Top Programs by Error Count Across Hosts",
       x = "Program", y = "Total Errors")

# 4. Scatter: error rate vs avg CPU (per program-host combo)
ggplot(program_errors, aes(x = avg_cpu, y = error_rate, color = Hostname, label = programs)) +
  geom_point(alpha = 0.7) +
  geom_text(aes(label = ifelse(error_rate > 0, programs, '')), hjust = 0, vjust = 0, sise = 3) +
  labs(title = "Error Rate vs Avg CPU Usage by Program and Host",
       x = "Avg CPU (%)", y = "Error Rate")

```

I also noticed that a high average CPU does not nessaserily mean error but programs do matter; i.e. lib vert has very low average cpu usage mostly, but still accounts for a lot of errors, alternatively neutron open switch ages has pretty high average cpu but very low error rate compared to libvert.

Maybe programs are more influential than CPU usage?

# ------------------------------------------------------------------------------

```{r}
traces <- read.csv(file.path("~/Desktop/DXC AI team assesment/data/processed/sequential_data/traces/combined_sequential_traces.csv"))   
```

```{r}
# Define the experiment interval
start_time <- as.POSIXct("2019-11-19 17:38:39", tz = "UTC")
end_time   <- as.POSIXct("2019-11-20 01:30:00", tz = "UTC")

traces_filtered <- traces %>%
  mutate(Timestamp = as.POSIXct(Timestamp, tz = "UTC")) %>%
  filter(Timestamp >= start_time & Timestamp <= end_time)
```

```{r}
traces_binned <- traces_filtered %>%
  mutate(time_bin = floor_date(Timestamp, unit = "minute"))  # 60s bins
```

```{r}
traces_agg <- traces_binned %>%
  group_by(Host, time_bin, operation) %>%
  summarise(total_traces = n(), .groups = "drop")
```

```{r}
traces_agg <- traces_agg %>%
  rename(
    Hostname = Host,
    bin_time = time_bin
  )
```


```{r}
# Combine traces_agg and combined
full_data <- full_join(combined, traces_agg, 
                       by = c("Hostname", "bin_time"))  # join on Hostname and bin_time
```

# Cleaning the data

```{r}
# Columns to convert
cols_to_factor <- c("Hostname", "user_ids", "programs", "pids", "operation")

# Convert to factors
full_data[cols_to_factor] <- lapply(full_data[cols_to_factor], factor)
```

```{r}
# Check for missingness in columns
colSums(is.na(full_data))
```

```{r}
# Check if the NA patterns are identical across these columns
cols_to_check <- c("total_logs", "error_logs", "warning_logs", "info_logs", "user_ids", "programs", "pids")
all_same_na <- apply(is.na(full_data[cols_to_check]), 1, all)
sum(all_same_na)  # Should be 1197 if all missing in the same rows


missing_rows <- full_data[all_same_na, ]
```


```{r}
library(dplyr)
library(ggplot2)
library(tidyr)

# Create a flag for missing data (based on total_logs, error_logs, etc.)
full_data <- full_data %>%
  mutate(missing_data = ifelse(is.na(total_logs), TRUE, FALSE))

# Pick only relevant columns for visualization
missing_plot_data <- full_data %>%
  select(Hostname, bin_time, missing_data)

# Plot heatmap of missing data
ggplot(missing_plot_data, aes(x = bin_time, y = Hostname, fill = missing_data)) +
  geom_tile(color = "white") +
  scale_fill_manual(values = c("TRUE" = "red", "FALSE" = "green"), 
                    labels = c("Missing", "Present")) +
  labs(title = "Missing Data per Host Over Time",
       x = "Timestamp",
       y = "Hostname",
       fill = "Data Status") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

* Missingness is perfectly aligned across total_logs, error_logs, warning_logs, info_logs, user_ids, programs, and pids.
* They just indicate “no activity for this host at this minute.”

```{r}
# operation and total_traces each have 1496 rows

# Are the missing rows the same?
sum(is.na(full_data$operation) & is.na(full_data$total_traces))

# Do these missing rows correspond to the rows where logs (total_logs, error_logs, etc.) are present or absent?
table(is.na(full_data$total_logs), is.na(full_data$operation))

# Check distinct operation values
unique(full_data$operation)
```

* Missing operation / total_traces is structural - only defined during experiments.
* I don’t need to impute; just keep in mind that NAs = “not part of any experiment”.

# Check the levels of each factor column

```{r}
head(levels(full_data$Hostname))
head(levels(full_data$user_ids))
head(levels(full_data$programs))
head(levels(full_data$pids))
head(levels(full_data$operation))
```

```{r}
# in user_ids, programs, and pids where multiple values are encoded in one string, split these columns into separate rows
full_data <- full_data %>%
  mutate(
    user_ids = strsplit(as.character(user_ids), ","),
    programs = strsplit(as.character(programs), ","),
    pids = strsplit(as.character(pids), ",")
  )

user_long <- full_data %>%
  select(-programs, -pids) %>%
  unnest(user_ids)

program_long <- full_data %>%
  select(-user_ids, -pids) %>%
  unnest(programs)

pid_long <- full_data %>%
  select(-user_ids, -programs) %>%
  unnest(pids)

```

```{r}
full_data_long <- full_data %>%
  mutate(combinations = pmap(list(user_ids, programs, pids),
                              ~ expand.grid(user_id = ..1, program = ..2, pid = ..3))) %>%
  unnest(combinations)
```


```{r}
full_data_lists <- full_data_long %>%
  mutate(num_users = map_int(user_ids, length),
         num_programs = map_int(programs, length),
         num_pids = map_int(pids, length))
```


```{r}
full_data_export <- full_data_lists %>%
  mutate(
    user_ids = sapply(user_ids, paste, collapse = ","),
    programs = sapply(programs, paste, collapse = ","),
    pids = sapply(pids, paste, collapse = ",")
  )

# write.csv(full_data_export, "full_data_lists.csv", row.names = FALSE)
```



```{r}
full_data_long <- full_data_long %>%
  select(-user_ids, -programs, -pids)

# Export to CSV
# write.csv(full_data_long, "full_data_long.csv", row.names = FALSE)
```


```{r}
# Get the min and max timestamps
min_time <- min(full_data_long$bin_time, na.rm = TRUE)
max_time <- max(full_data_long$bin_time, na.rm = TRUE)

# Generate a complete sequence of minutes
complete_minutes <- seq(from = floor_date(min_time, "minute"),
                        to = ceiling_date(max_time, "minute"),
                        by = "1 min")

# Compare with actual unique minutes in the data
actual_minutes <- sort(unique(full_data_long$bin_time))

# Find missing minutes
missing_minutes <- setdiff(complete_minutes, actual_minutes)
length(missing_minutes)  # how many minutes are missing
missing_minutes          # show the missing timestamps

```

```{r}
rows_at_midnight <- full_data_long[hour(full_data_long$bin_time) == 0 &
                                   minute(full_data_long$bin_time) == 0 &
                                   second(full_data_long$bin_time) == 0, ]

nrow(rows_at_midnight)  # number of rows at 12 AM
head(rows_at_midnight)  # preview a few rows
```




```{r}
# Identify midnight rows
full_data_long <- full_data_long %>%
  mutate(is_midnight = hour(bin_time) == 0 & minute(bin_time) == 0 & second(bin_time) == 0)

# Split the data
data_midnight <- full_data_long %>% filter(is_midnight)
data_no_midnight <- full_data_long %>% filter(!is_midnight)

# Function to plot total_logs over time per host
plot_logs_over_time <- function(df, title){
  ggplot(df, aes(x = bin_time, y = total_logs, color = Hostname)) +
    geom_line() +
    geom_point() +
    labs(title = title, x = "Time", y = "Total Logs") +
    theme_minimal() +
    theme(legend.position = "bottom")
}

# Plot including midnight rows
plot_logs_over_time(full_data_long, "Total Logs Over Time (Including Midnight Rows)")

# Plot excluding midnight rows
plot_logs_over_time(data_no_midnight, "Total Logs Over Time (Excluding Midnight Rows)")

```

```{r}
# Add a midnight indicator if not already present
full_data_long <- full_data_long %>%
  mutate(is_midnight = hour(bin_time) == 0 & minute(bin_time) == 0 & second(bin_time) == 0)

# Summary statistics function
summary_stats <- function(df, group_name) {
  df %>%
    summarise(
      n = n(),
      mean_total_logs = mean(total_logs, na.rm = TRUE),
      sd_total_logs = sd(total_logs, na.rm = TRUE),
      mean_error_logs = mean(error_logs, na.rm = TRUE),
      sd_error_logs = sd(error_logs, na.rm = TRUE),
      mean_warning_logs = mean(warning_logs, na.rm = TRUE),
      sd_warning_logs = sd(warning_logs, na.rm = TRUE),
      mean_info_logs = mean(info_logs, na.rm = TRUE),
      sd_info_logs = sd(info_logs, na.rm = TRUE),
      mean_cpu_user = mean(cpu_user_mean, na.rm = TRUE),
      sd_cpu_user = sd(cpu_user_mean, na.rm = TRUE),
      mean_mem_used = mean(mem_used_mean, na.rm = TRUE),
      sd_mem_used = sd(mem_used_mean, na.rm = TRUE)
    ) %>%
    mutate(group = group_name)
}

# Midnight rows stats
midnight_stats <- summary_stats(full_data_long %>% filter(is_midnight), "Midnight Rows")

# Non-midnight rows stats
non_midnight_stats <- summary_stats(full_data_long %>% filter(!is_midnight), "Non-Midnight Rows")

# Combine
stats_comparison <- bind_rows(midnight_stats, non_midnight_stats)
stats_comparison
```


# ------------------------------------------------------------------------------

# add events data

```{r}
reports <- read.csv("~/Desktop/DXC AI team assesment/data/processed/sequential_data/reports/flattened_sequential_reports.csv")
```

# ------------------------------------------------------------------------------

```{r}
# Summarise total logs and error logs per program
program_summary <- full_data_lists %>%
  mutate(programs = map(programs, ~ as.character(.x))) %>%  # ensure character
  unnest(programs) %>%  # unnest program list only
  group_by(programs) %>%
  summarise(
    total_logs = sum(total_logs, na.rm = TRUE),
    error_logs = sum(error_logs, na.rm = TRUE),
    error_rate = error_logs / total_logs
  ) %>%
  arrange(desc(error_rate))

# View top programs by error rate
head(program_summary)
```


```{r}
user_summary <- full_data_lists %>%
  mutate(user_ids = map(user_ids, ~ as.character(.x))) %>%
  unnest(user_ids) %>%
  group_by(user_ids) %>%
  summarise(
    total_logs = sum(total_logs, na.rm = TRUE),
    error_logs = sum(error_logs, na.rm = TRUE),
    error_rate = error_logs / total_logs
  ) %>%
  arrange(desc(error_rate))

head(user_summary)
```

