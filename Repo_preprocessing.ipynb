{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben-velastegui/dxc-ai-assessment/blob/main/Repo_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mo4hAa0DkdL"
      },
      "source": [
        "# Install packages & load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7odYWEtfyaee",
        "outputId": "a6e09ffd-1bf5-4c8b-8ae4-560d43d05391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas matplotlib seaborn PyPDF2\n",
        "\n",
        "import sys # Provides access to runtime environment variables\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import os, json, pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDoAcGCiDpja"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AUyHu1rx0bT_",
        "outputId": "5eedc10d-b35a-43b7-955f-4ab8d22807f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-12 15:28:28--  https://zenodo.org/record/3549604/files/concurrent%20data.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/3549604/files/concurrent%20data.zip [following]\n",
            "--2025-09-12 15:28:29--  https://zenodo.org/records/3549604/files/concurrent%20data.zip\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 335390038 (320M) [application/octet-stream]\n",
            "Saving to: â€˜concurrent_data.zipâ€™\n",
            "\n",
            "concurrent_data.zip 100%[===================>] 319.85M  14.1MB/s    in 5m 20s  \n",
            "\n",
            "2025-09-12 15:33:49 (1.00 MB/s) - â€˜concurrent_data.zipâ€™ saved [335390038/335390038]\n",
            "\n",
            "--2025-09-12 15:33:49--  https://zenodo.org/record/3549604/files/sequential_data.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.45.92, 188.185.43.25, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/3549604/files/sequential_data.zip [following]\n",
            "--2025-09-12 15:33:50--  https://zenodo.org/records/3549604/files/sequential_data.zip\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 315104962 (301M) [application/octet-stream]\n",
            "Saving to: â€˜sequential_data.zipâ€™\n",
            "\n",
            "sequential_data.zip 100%[===================>] 300.51M   436KB/s    in 11m 54s \n",
            "\n",
            "2025-09-12 15:45:44 (431 KB/s) - â€˜sequential_data.zipâ€™ saved [315104962/315104962]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the zip files with the Zenodo data into Colab\n",
        "!wget -O concurrent_data.zip https://zenodo.org/record/3549604/files/concurrent%20data.zip\n",
        "!wget -O sequential_data.zip https://zenodo.org/record/3549604/files/sequential_data.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVTnwuNh5Vhv"
      },
      "outputs": [],
      "source": [
        "# Unzip the data\n",
        "!unzip -q concurrent_data.zip -d /content/\n",
        "!unzip -q sequential_data.zip -d /content/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gIfXM-xX_tj"
      },
      "source": [
        "## Map folder structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DelS_kTmTRVO",
        "outputId": "717fd586-4064-42e3-b69a-569ca19dc09f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "concurrent_data structure:\n",
            "\n",
            "â”œâ”€â”€ IMPORTANT_experiment_start_end_data.txt\n",
            "â”œâ”€â”€ logs/\n",
            "â”‚   â”œâ”€â”€ logs_aggregated_concurrent.csv\n",
            "â”‚   â”œâ”€â”€ wally113_logs.tar.gz\n",
            "â”‚   â”œâ”€â”€ wally117_logs.tar.gz\n",
            "â”‚   â”œâ”€â”€ wally122_logs.tar.gz\n",
            "â”‚   â”œâ”€â”€ wally123_logs.tar.gz\n",
            "â”‚   â””â”€â”€ wally124_logs.tar.gz\n",
            "â”œâ”€â”€ metrics/\n",
            "â”‚   â”œâ”€â”€ wally113_metrics_concurrent.csv\n",
            "â”‚   â”œâ”€â”€ wally117_metrics_concurrent.csv\n",
            "â”‚   â”œâ”€â”€ wally122_metrics_concurrent.csv\n",
            "â”‚   â”œâ”€â”€ wally123_metrics_concurrent.csv\n",
            "â”‚   â””â”€â”€ wally124_metrics_concurrent.csv\n",
            "â”œâ”€â”€ reports/\n",
            "â”‚   â”œâ”€â”€ output_boot.html\n",
            "â”‚   â”œâ”€â”€ output_boot.json\n",
            "â”‚   â”œâ”€â”€ output_image.html\n",
            "â”‚   â”œâ”€â”€ output_image.json\n",
            "â”‚   â”œâ”€â”€ output_network.html\n",
            "â”‚   â””â”€â”€ output_network.json\n",
            "â””â”€â”€ traces/\n",
            "    â”œâ”€â”€ boot_delete/ (2000 files)\n",
            "    â”œâ”€â”€ image_create_delete/ (3000 files)\n",
            "    â””â”€â”€ network_create_delete/ (6000 files)\n",
            "\n",
            "==================================================\n",
            "\n",
            "sequential_data structure:\n",
            "\n",
            "â”œâ”€â”€ IMPORTANT_experiment_start_end_data.txt\n",
            "â”œâ”€â”€ logs/\n",
            "â”‚   â”œâ”€â”€ logs_aggregated_sequential.csv\n",
            "â”‚   â”œâ”€â”€ wally113_logs.rar\n",
            "â”‚   â”œâ”€â”€ wally117_logs.zip\n",
            "â”‚   â”œâ”€â”€ wally122_logs.rar\n",
            "â”‚   â”œâ”€â”€ wally123_logs.zip\n",
            "â”‚   â””â”€â”€ wally124_logs.zip\n",
            "â”œâ”€â”€ metrics/\n",
            "â”‚   â”œâ”€â”€ wally113_metrics.csv\n",
            "â”‚   â”œâ”€â”€ wally117_metrics.csv\n",
            "â”‚   â”œâ”€â”€ wally122_metrics.csv\n",
            "â”‚   â”œâ”€â”€ wally123_metrics.csv\n",
            "â”‚   â””â”€â”€ wally124_metrics.csv\n",
            "â”œâ”€â”€ reports/\n",
            "â”‚   â”œâ”€â”€ j_boot_delete_report.html\n",
            "â”‚   â”œâ”€â”€ j_boot_delete_report.json\n",
            "â”‚   â”œâ”€â”€ j_image_report.html\n",
            "â”‚   â”œâ”€â”€ j_image_report.json\n",
            "â”‚   â”œâ”€â”€ j_network_output.html\n",
            "â”‚   â””â”€â”€ j_network_output.json\n",
            "â””â”€â”€ traces/\n",
            "    â”œâ”€â”€ boot_delete/ (750 files)\n",
            "    â”œâ”€â”€ image_create_delete/ (1000 files)\n",
            "    â””â”€â”€ network_create_delete/ (1000 files)\n"
          ]
        }
      ],
      "source": [
        "# Create a measure to visualise a tree map for the file structure\n",
        "import os\n",
        "\n",
        "def print_tree(root_dir, exclude_dirs=None, indent=\"\"):\n",
        "    if exclude_dirs is None:\n",
        "        exclude_dirs = []\n",
        "\n",
        "    # List items\n",
        "    items = sorted(os.listdir(root_dir))\n",
        "    for i, item in enumerate(items):\n",
        "        path = os.path.join(root_dir, item)\n",
        "        connector = \"â””â”€â”€ \" if i == len(items) - 1 else \"â”œâ”€â”€ \"\n",
        "\n",
        "        if os.path.isdir(path):\n",
        "            if item in exclude_dirs:\n",
        "                # Count files instead of listing them\n",
        "                num_files = sum([len(files) for _, _, files in os.walk(path)])\n",
        "                print(f\"{indent}{connector}{item}/ ({num_files} files)\")\n",
        "            else:\n",
        "                print(f\"{indent}{connector}{item}/\")\n",
        "                # Recurse\n",
        "                print_tree(path, exclude_dirs, indent + (\"    \" if i == len(items)-1 else \"â”‚   \"))\n",
        "        else:\n",
        "            print(f\"{indent}{connector}{item}\")\n",
        "\n",
        "# Generate tree for both datasets\n",
        "print(\"concurrent_data structure:\\n\")\n",
        "print_tree(\"concurrent data\", exclude_dirs=[\"boot_delete\", \"image_create_delete\", \"network_create_delete\"])\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"sequential_data structure:\\n\")\n",
        "print_tree(\"sequential_data\", exclude_dirs=[\"boot_delete\", \"image_create_delete\", \"network_create_delete\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxaVoDrFD3DA"
      },
      "source": [
        "# Clone the GitHub repo with the helper scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "G5hCbpTy5wpZ",
        "outputId": "e9baa29e-646f-48d6-b27b-6c657ff0c3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'multi-source-observability-dataset'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 107 (delta 4), reused 1 (delta 1), pack-reused 95 (from 1)\u001b[K\n",
            "Receiving objects: 100% (107/107), 1.23 MiB | 11.28 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n"
          ]
        }
      ],
      "source": [
        "# The Zenodo data authors specify a GitHub repo with helper scripts for data exploration\n",
        "# Clone the GitHub repo\n",
        "!git clone https://github.com/SashoNedelkoski/multi-source-observability-dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gDkQvhzYnrz",
        "outputId": "bc15f733-417f-4fdc-b8c0-90134b5f6997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "multi-source-observability-dataset:\n",
            "\n",
            "â”œâ”€â”€ .git/\n",
            "â”‚   â”œâ”€â”€ HEAD\n",
            "â”‚   â”œâ”€â”€ branches/\n",
            "â”‚   â”œâ”€â”€ config\n",
            "â”‚   â”œâ”€â”€ description\n",
            "â”‚   â”œâ”€â”€ hooks/\n",
            "â”‚   â”‚   â”œâ”€â”€ applypatch-msg.sample\n",
            "â”‚   â”‚   â”œâ”€â”€ commit-msg.sample\n",
            "â”‚   â”‚   â”œâ”€â”€ fsmonitor-watchman.sample\n",
            "â”‚   â”‚   â”œâ”€â”€ post-update.sample\n",
            "â”‚   â”‚   â”œâ”€â”€ pre-applypatch.sample\n",
            "â”‚   â”‚   â”œâ”€â”€ pre-commit.sample\n",
            "â”‚   â”‚   â”œâ”€â”€ pre-merge-commit.sample\n",
            "â”‚   â”‚   â”œâ”€â”€ pre-push.sample\n",
            "â”‚   â”‚   â”œâ”€â”€ pre-rebase.sample\n",
            "â”‚   â”‚   â”œâ”€â”€ pre-receive.sample\n",
            "â”‚   â”‚   â”œâ”€â”€ prepare-commit-msg.sample\n",
            "â”‚   â”‚   â”œâ”€â”€ push-to-checkout.sample\n",
            "â”‚   â”‚   â””â”€â”€ update.sample\n",
            "â”‚   â”œâ”€â”€ index\n",
            "â”‚   â”œâ”€â”€ info/\n",
            "â”‚   â”‚   â””â”€â”€ exclude\n",
            "â”‚   â”œâ”€â”€ logs/\n",
            "â”‚   â”‚   â”œâ”€â”€ HEAD\n",
            "â”‚   â”‚   â””â”€â”€ refs/\n",
            "â”‚   â”‚       â”œâ”€â”€ heads/\n",
            "â”‚   â”‚       â”‚   â””â”€â”€ master\n",
            "â”‚   â”‚       â””â”€â”€ remotes/\n",
            "â”‚   â”‚           â””â”€â”€ origin/\n",
            "â”‚   â”‚               â””â”€â”€ HEAD\n",
            "â”‚   â”œâ”€â”€ objects/\n",
            "â”‚   â”‚   â”œâ”€â”€ info/\n",
            "â”‚   â”‚   â””â”€â”€ pack/\n",
            "â”‚   â”‚       â”œâ”€â”€ pack-ff1f58c294340a6c155b929b56828f4967e53a47.idx\n",
            "â”‚   â”‚       â””â”€â”€ pack-ff1f58c294340a6c155b929b56828f4967e53a47.pack\n",
            "â”‚   â”œâ”€â”€ packed-refs\n",
            "â”‚   â””â”€â”€ refs/\n",
            "â”‚       â”œâ”€â”€ heads/\n",
            "â”‚       â”‚   â””â”€â”€ master\n",
            "â”‚       â”œâ”€â”€ remotes/\n",
            "â”‚       â”‚   â””â”€â”€ origin/\n",
            "â”‚       â”‚       â””â”€â”€ HEAD\n",
            "â”‚       â””â”€â”€ tags/\n",
            "â”œâ”€â”€ README.md\n",
            "â”œâ”€â”€ investigateMetrics.py\n",
            "â”œâ”€â”€ investigateTraces.py\n",
            "â”œâ”€â”€ reading.py\n",
            "â””â”€â”€ workloads/\n",
            "    â”œâ”€â”€ boot_delete_config/\n",
            "    â”‚   â”œâ”€â”€ boot-and-delete.yaml\n",
            "    â”‚   â”œâ”€â”€ commands-to-exec-nova-compute.sh\n",
            "    â”‚   â”œâ”€â”€ commands-to-exec-nova.sh\n",
            "    â”‚   â”œâ”€â”€ j_boot_delete_output.json\n",
            "    â”‚   â”œâ”€â”€ restart_nova_compute_nodes.sh\n",
            "    â”‚   â”œâ”€â”€ restart_nova_container.sh\n",
            "    â”‚   â”œâ”€â”€ trace_to_csv.py\n",
            "    â”‚   â”œâ”€â”€ trace_to_csv_1.1.py\n",
            "    â”‚   â””â”€â”€ traceids_j_boot_delete.txt\n",
            "    â”œâ”€â”€ create_delete_image_config/\n",
            "    â”‚   â”œâ”€â”€ commands-to-exec-glance.sh\n",
            "    â”‚   â”œâ”€â”€ create-and-delete-image.yaml\n",
            "    â”‚   â”œâ”€â”€ j_image_output.html\n",
            "    â”‚   â”œâ”€â”€ j_image_output.json\n",
            "    â”‚   â”œâ”€â”€ list-images.yml\n",
            "    â”‚   â”œâ”€â”€ output_jasmin_image.html\n",
            "    â”‚   â”œâ”€â”€ output_jasmin_image.json\n",
            "    â”‚   â”œâ”€â”€ output_jasmin_images.json\n",
            "    â”‚   â”œâ”€â”€ restart_glance_container.sh\n",
            "    â”‚   â”œâ”€â”€ test.json\n",
            "    â”‚   â”œâ”€â”€ trace_ids_18th_nov\n",
            "    â”‚   â”œâ”€â”€ traceids_j_image.txt\n",
            "    â”‚   â””â”€â”€ traceids_jasmin.txt\n",
            "    â”œâ”€â”€ json_trace_ids.py\n",
            "    â”œâ”€â”€ keystone_auth.yaml\n",
            "    â”œâ”€â”€ network_create_delete_config/\n",
            "    â”‚   â”œâ”€â”€ commands-to-exec-neutron.sh\n",
            "    â”‚   â”œâ”€â”€ create-and-delete-networks.yaml\n",
            "    â”‚   â”œâ”€â”€ debug.txt\n",
            "    â”‚   â”œâ”€â”€ glances_out.csv\n",
            "    â”‚   â”œâ”€â”€ restart_neutron_container.sh\n",
            "    â”‚   â””â”€â”€ traceids_j_network.txt\n",
            "    â””â”€â”€ os-faults-latest.yaml\n"
          ]
        }
      ],
      "source": [
        "# Create a measure to visualise a tree map for the file structure\n",
        "import os\n",
        "\n",
        "def print_tree(root_dir, exclude_dirs=None, indent=\"\"):\n",
        "    if exclude_dirs is None:\n",
        "        exclude_dirs = []\n",
        "\n",
        "    # List items\n",
        "    items = sorted(os.listdir(root_dir))\n",
        "    for i, item in enumerate(items):\n",
        "        path = os.path.join(root_dir, item)\n",
        "        connector = \"â””â”€â”€ \" if i == len(items) - 1 else \"â”œâ”€â”€ \"\n",
        "\n",
        "        if os.path.isdir(path):\n",
        "            if item in exclude_dirs:\n",
        "                # Count files instead of listing them\n",
        "                num_files = sum([len(files) for _, _, files in os.walk(path)])\n",
        "                print(f\"{indent}{connector}{item}/ ({num_files} files)\")\n",
        "            else:\n",
        "                print(f\"{indent}{connector}{item}/\")\n",
        "                # Recurse\n",
        "                print_tree(path, exclude_dirs, indent + (\"    \" if i == len(items)-1 else \"â”‚   \"))\n",
        "        else:\n",
        "            print(f\"{indent}{connector}{item}\")\n",
        "\n",
        "# Generate tree for both datasets\n",
        "print(\"multi-source-observability-dataset:\\n\")\n",
        "print_tree(\"multi-source-observability-dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I assume that the files in the /git/ folder are Gitâ€™s version control system related which you can mostly ignore this when analyzing the dataset.\n",
        "\n",
        "Therefore I will understand the repo by going through a subsect of each othe contents of the files which are not in the /git/ folder.\n",
        "\n",
        "\n",
        "## Investigate the following files from the repo"
      ],
      "metadata": {
        "id": "IA_1YMFplLvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a subsect of each file from non-git folders in a txt file\n",
        "# Base path to repo\n",
        "base = \"/content/multi-source-observability-dataset\"\n",
        "\n",
        "# List of files to inspect\n",
        "files = [\n",
        "    \"README.md\",\n",
        "    \"investigateMetrics.py\",\n",
        "    \"investigateTraces.py\",\n",
        "    \"reading.py\",\n",
        "    \"workloads/boot_delete_config/boot-and-delete.yaml\",\n",
        "    \"workloads/boot_delete_config/commands-to-exec-nova-compute.sh\",\n",
        "    \"workloads/boot_delete_config/commands-to-exec-nova.sh\",\n",
        "    \"workloads/boot_delete_config/j_boot_delete_output.json\",\n",
        "    \"workloads/boot_delete_config/restart_nova_compute_nodes.sh\",\n",
        "    \"workloads/boot_delete_config/restart_nova_container.sh\",\n",
        "    \"workloads/boot_delete_config/trace_to_csv.py\",\n",
        "    \"workloads/boot_delete_config/trace_to_csv_1.1.py\",\n",
        "    \"workloads/boot_delete_config/traceids_j_boot_delete.txt\",\n",
        "    \"workloads/create_delete_image_config/commands-to-exec-glance.sh\",\n",
        "    \"workloads/create_delete_image_config/create-and-delete-image.yaml\",\n",
        "    \"workloads/create_delete_image_config/j_image_output.html\",\n",
        "    \"workloads/create_delete_image_config/j_image_output.json\",\n",
        "    \"workloads/create_delete_image_config/list-images.yml\",\n",
        "    \"workloads/create_delete_image_config/output_jasmin_image.html\",\n",
        "    \"workloads/create_delete_image_config/output_jasmin_image.json\",\n",
        "    \"workloads/create_delete_image_config/output_jasmin_images.json\",\n",
        "    \"workloads/create_delete_image_config/restart_glance_container.sh\",\n",
        "    \"workloads/create_delete_image_config/test.json\",\n",
        "    \"workloads/create_delete_image_config/trace_ids_18th_nov\",\n",
        "    \"workloads/create_delete_image_config/traceids_j_image.txt\",\n",
        "    \"workloads/create_delete_image_config/traceids_jasmin.txt\",\n",
        "    \"workloads/json_trace_ids.py\",\n",
        "    \"workloads/keystone_auth.yaml\",\n",
        "    \"workloads/network_create_delete_config/commands-to-exec-neutron.sh\",\n",
        "    \"workloads/network_create_delete_config/create-and-delete-networks.yaml\",\n",
        "    \"workloads/network_create_delete_config/debug.txt\",\n",
        "    \"workloads/network_create_delete_config/glances_out.csv\",\n",
        "    \"workloads/network_create_delete_config/restart_neutron_container.sh\",\n",
        "    \"workloads/network_create_delete_config/traceids_j_network.txt\",\n",
        "    \"workloads/os-faults-latest.yaml\",\n",
        "]\n",
        "\n",
        "output_file = \"/content/file_previews.txt\"\n",
        "\n",
        "def preview_file(path):\n",
        "    \"\"\"Return a safe preview of a file as a string.\"\"\"\n",
        "    result = []\n",
        "    result.append(\"=\"*80)\n",
        "    result.append(f\"ğŸ“„ {path}\")\n",
        "    result.append(\"=\"*80)\n",
        "    try:\n",
        "        if path.endswith(\".json\"):\n",
        "            with open(path, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "            result.append(json.dumps(data, indent=2)[:2000])\n",
        "        elif path.endswith(\".csv\"):\n",
        "            df = pd.read_csv(path)\n",
        "            result.append(str(df.head()))\n",
        "        else:\n",
        "            with open(path, \"r\", errors=\"ignore\") as f:\n",
        "                text = f.read(2000)\n",
        "            result.append(text)\n",
        "    except Exception as e:\n",
        "        result.append(f\"âš ï¸ Could not open {path}: {e}\")\n",
        "    return \"\\n\".join(result)\n",
        "\n",
        "# Collect previews\n",
        "all_previews = []\n",
        "for f in files:\n",
        "    all_previews.append(preview_file(os.path.join(base, f)))\n",
        "\n",
        "# Save into one file\n",
        "with open(output_file, \"w\") as out:\n",
        "    out.write(\"\\n\\n\".join(all_previews))\n",
        "\n",
        "print(f\"âœ… Previews saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "OLoRhn5qlJOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oLoQUtqGE8L"
      },
      "source": [
        "Everything else in workloads/ â€” the YAMLs (boot-and-delete.yaml, create-and-delete-image.yaml, etc.) and shell scripts (commands-to-exec-nova.sh, restart_glance_container.sh, etc.) â€” were used by the original authors to:\n",
        "\n",
        "* Deploy workloads against their OpenStack testbed,\n",
        "* Restart services after injecting faults,\n",
        "* Record new traces via Jaeger.\n",
        "\n",
        "Since the Zenodo dataset already includes the raw JSON traces, you donâ€™t need to touch those workload configs/scripts unless youâ€™re trying to reproduce the experiments on a live OpenStack cloud (which is not my goal).\n",
        "\n",
        "The only relevant scripts inside workloads/ are:\n",
        "\n",
        "* trace_to_csv.py\n",
        "* trace_to_csv_1.1.py\n",
        "\n",
        "These are the parsing utilities that take the JSON traces and generate CSVs. Those CSVs are what investigateTraces.py and investigateMetrics.py work on.\n",
        "\n",
        "\n",
        "# Run the helper scripts provided in the Github Repo\n",
        "\n",
        "## investigateMetrics.py for the sequential data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__Y7miNDNuRX"
      },
      "outputs": [],
      "source": [
        "# Path to the original script\n",
        "script_path = \"/content/multi-source-observability-dataset/investigateMetrics.py\"\n",
        "\n",
        "# Read the script\n",
        "with open(script_path) as f:\n",
        "    code = f.read()\n",
        "\n",
        "# Extract everything up to (but not including) the bottom for-loop\n",
        "# We'll assume functions start with 'def ' and everything else we want to skip\n",
        "functions_only = []\n",
        "inside_function = False\n",
        "for line in code.splitlines():\n",
        "    if line.strip().startswith(\"for nodeID in\"):\n",
        "        break  # stop before the script runs its loop\n",
        "    functions_only.append(line)\n",
        "\n",
        "functions_code = \"\\n\".join(functions_only)\n",
        "exec(functions_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSPJxXwYOKnZ",
        "outputId": "0d18cc87-4904-4ed1-e815-3c1d0cf94596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stats for node 113:\n",
            "  cpu.user: {'mean': np.float64(19.97177685950413), 'median': np.float64(14.2), 'std': 12.405052218118247, 'min': 3.1, 'max': 82.2}\n",
            "  mem.used: {'mean': np.float64(13480905215.90597), 'median': np.float64(13730633728.0), 'std': 1138441344.7515936, 'min': 9962508288, 'max': 16643276800}\n",
            "  load.cpucore: {'mean': np.float64(8.0), 'median': np.float64(8.0), 'std': 0.0, 'min': 8, 'max': 8}\n",
            "  load.min1: {'mean': np.float64(3.3474869605142334), 'median': np.float64(2.99), 'std': 4.79198689403737, 'min': 0.8, 'max': 365.2}\n",
            "  load.min5: {'mean': np.float64(3.5650289256198344), 'median': np.float64(3.03), 'std': 6.039353974391843, 'min': 1.02, 'max': 201.03}\n",
            "  load.min15: {'mean': np.float64(3.66319797979798), 'median': np.float64(2.95), 'std': 4.655557047261218, 'min': 1.18, 'max': 87.92}\n",
            "Stats for node 117:\n",
            "  cpu.user: {'mean': np.float64(14.849229005099733), 'median': np.float64(14.1), 'std': 2.8202836951985986, 'min': 8.7, 'max': 49.9}\n",
            "  mem.used: {'mean': np.float64(3310395161.9518323), 'median': np.float64(3319676928.0), 'std': 77339720.1070996, 'min': 2917302272, 'max': 3814244352}\n",
            "  load.cpucore: {'mean': np.float64(8.0), 'median': np.float64(8.0), 'std': 0.0, 'min': 8, 'max': 8}\n",
            "  load.min1: {'mean': np.float64(1.6662296186768863), 'median': np.float64(1.51), 'std': 0.46732066506580827, 'min': 0.31, 'max': 4.7}\n",
            "  load.min5: {'mean': np.float64(1.6614942112515967), 'median': np.float64(1.53), 'std': 0.4170647701537176, 'min': 0.82, 'max': 3.41}\n",
            "  load.min15: {'mean': np.float64(1.653008673902183), 'median': np.float64(1.52), 'std': 0.39221120081090083, 'min': 1.22, 'max': 3.1}\n",
            "Stats for node 122:\n",
            "  cpu.user: {'mean': np.float64(14.663408562955416), 'median': np.float64(14.3), 'std': 2.7481050917904724, 'min': 8.3, 'max': 50.6}\n",
            "  mem.used: {'mean': np.float64(2531378694.0338516), 'median': np.float64(2573303808.0), 'std': 112405155.02217798, 'min': 1973506048, 'max': 3061833728}\n",
            "  load.cpucore: {'mean': np.float64(8.0), 'median': np.float64(8.0), 'std': 0.0, 'min': 8, 'max': 8}\n",
            "  load.min1: {'mean': np.float64(1.611539197329109), 'median': np.float64(1.47), 'std': 0.42057276470189775, 'min': 0.22, 'max': 4.53}\n",
            "  load.min5: {'mean': np.float64(1.6064221796278033), 'median': np.float64(1.48), 'std': 0.3658494745807779, 'min': 0.91, 'max': 3.3}\n",
            "  load.min15: {'mean': np.float64(1.596991184432602), 'median': np.float64(1.47), 'std': 0.3410829682659557, 'min': 1.21, 'max': 3.04}\n",
            "Stats for node 123:\n",
            "  cpu.user: {'mean': np.float64(26.503781561854563), 'median': np.float64(26.4), 'std': 2.755319535191617, 'min': 12.4, 'max': 74.0}\n",
            "  mem.used: {'mean': np.float64(4824207547.142787), 'median': np.float64(5290074112.0), 'std': 908687173.4708211, 'min': 2379153408, 'max': 6014218240}\n",
            "  load.cpucore: {'mean': np.float64(8.0), 'median': np.float64(8.0), 'std': 0.0, 'min': 8, 'max': 8}\n",
            "  load.min1: {'mean': np.float64(2.7775939226630992), 'median': np.float64(2.63), 'std': 0.4662835353489604, 'min': 1.59, 'max': 5.38}\n",
            "  load.min5: {'mean': np.float64(2.7707872325806058), 'median': np.float64(2.65), 'std': 0.4077859545785003, 'min': 1.57, 'max': 4.53}\n",
            "  load.min15: {'mean': np.float64(2.754050668725319), 'median': np.float64(2.64), 'std': 0.39321839380502993, 'min': 1.49, 'max': 4.21}\n",
            "Stats for node 124:\n",
            "  cpu.user: {'mean': np.float64(14.660364996448443), 'median': np.float64(14.3), 'std': 2.665820917018509, 'min': 5.7, 'max': 47.9}\n",
            "  mem.used: {'mean': np.float64(2426559272.141115), 'median': np.float64(2460442624.0), 'std': 106672352.09853663, 'min': 1888342016, 'max': 3135143936}\n",
            "  load.cpucore: {'mean': np.float64(8.0), 'median': np.float64(8.0), 'std': 0.0, 'min': 8, 'max': 8}\n",
            "  load.min1: {'mean': np.float64(1.5926885284716463), 'median': np.float64(1.46), 'std': 0.3852901304751824, 'min': 0.4, 'max': 3.73}\n",
            "  load.min5: {'mean': np.float64(1.5876455028412455), 'median': np.float64(1.48), 'std': 0.32864607734235024, 'min': 1.04, 'max': 2.97}\n",
            "  load.min15: {'mean': np.float64(1.5782762371256065), 'median': np.float64(1.46), 'std': 0.3037805817969569, 'min': 1.31, 'max': 2.71}\n"
          ]
        }
      ],
      "source": [
        "metrics_path = \"/content/sequential_data/metrics\"\n",
        "\n",
        "for nodeID in [113, 117, 122, 123, 124]:\n",
        "    csv_file = os.path.join(metrics_path, f\"wally{nodeID}_metrics.csv\")\n",
        "    metricData = readMetrics(nodeID, csv_file)\n",
        "    plotFeaturesDistributionsMetrics(nodeID, metricData)\n",
        "\n",
        "    stats = caluclateStatisticalProperties(metricData)\n",
        "    print(f\"Stats for node {nodeID}:\")\n",
        "    for k, v in stats.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "F_giE4MKOkJD",
        "outputId": "c7249ef8-44e3-4ea7-940b-4658179ad223"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_fdc7cf8f-c45c-4c1f-ba86-81683bfb882e\", \"Histograms.zip\", 69793)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download output PDFs locally\n",
        "# Create a folder to store all output PDFs\n",
        "output_folder = \"Histograms\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Move all generated PDFs into this folder\n",
        "for nodeID in [113, 117, 122, 123, 124]:\n",
        "    pdf_name = f\"{nodeID}_new.pdf\"\n",
        "    if os.path.exists(pdf_name):\n",
        "        shutil.move(pdf_name, os.path.join(output_folder, pdf_name))\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(output_folder, 'zip', output_folder)\n",
        "\n",
        "# Download the zip to this local machine\n",
        "files.download(f\"{output_folder}.zip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCwgpFwpKIFJ"
      },
      "source": [
        "## investigateMetrics.py for the concurrent data\n",
        "\n",
        "Note: The investigateMetrics.py script references metric files using the naming convention [nodeID]_metrics.csv, which matches the filenames in the sequential data. In contrast, the concurrent data files follow the [nodeID]_metrics_concurrent.csv pattern. This suggests that the script was originally intended for use with the sequential dataset. While it is possible to adapt the script for the concurrent data by modifying the file paths, this should be considered a contextual adjustment rather than its default usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHzoGalyPkmS",
        "outputId": "9f7e4137-d556-4521-b4ee-913ede920b4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stats for node 113:\n",
            "  cpu.user: {'mean': np.float64(28.46906159366858), 'median': np.float64(24.9), 'std': 9.673930147262583, 'min': 8.7, 'max': 87.4}\n",
            "  mem.used: {'mean': np.float64(14831341022.168688), 'median': np.float64(14868291584.0), 'std': 357511964.18180126, 'min': 11231817728, 'max': 15755284480}\n",
            "  load.cpucore: {'mean': np.float64(8.0), 'median': np.float64(8.0), 'std': 0.0, 'min': 8, 'max': 8}\n",
            "  load.min1: {'mean': np.float64(4.062483163741829), 'median': np.float64(3.89), 'std': 1.3027021207344327, 'min': 1.18, 'max': 12.48}\n",
            "  load.min5: {'mean': np.float64(4.056931376886397), 'median': np.float64(3.69), 'std': 1.2191963309162668, 'min': 1.55, 'max': 7.9}\n",
            "  load.min15: {'mean': np.float64(4.040736961116846), 'median': np.float64(3.56), 'std': 1.1829858730968121, 'min': 1.83, 'max': 6.99}\n",
            "Stats for node 117:\n",
            "  cpu.user: {'mean': np.float64(11.306267753352008), 'median': np.float64(11.3), 'std': 1.193736489758931, 'min': 3.0, 'max': 47.6}\n",
            "  mem.used: {'mean': np.float64(3129303357.3123536), 'median': np.float64(3146977280.0), 'std': 301961383.40796196, 'min': 1872621568, 'max': 5748539392}\n",
            "  load.cpucore: {'mean': np.float64(8.0), 'median': np.float64(8.0), 'std': 0.0, 'min': 8, 'max': 8}\n",
            "  load.min1: {'mean': np.float64(1.198822646670178), 'median': np.float64(1.12), 'std': 0.30640256074976985, 'min': 0.91, 'max': 3.37}\n",
            "  load.min5: {'mean': np.float64(1.189358864894335), 'median': np.float64(1.12), 'std': 0.28882335688627064, 'min': 1.0, 'max': 2.66}\n",
            "  load.min15: {'mean': np.float64(1.160666749831361), 'median': np.float64(1.09), 'std': 0.27219902449785927, 'min': 0.63, 'max': 2.49}\n",
            "Stats for node 122:\n",
            "  cpu.user: {'mean': np.float64(11.38726778043369), 'median': np.float64(11.4), 'std': 1.1714082153738805, 'min': 1.5, 'max': 40.9}\n",
            "  mem.used: {'mean': np.float64(3148166199.699223), 'median': np.float64(3174866944.0), 'std': 298505507.99026346, 'min': 1852669952, 'max': 5801472000}\n",
            "  load.cpucore: {'mean': np.float64(8.0), 'median': np.float64(8.0), 'std': 0.0, 'min': 8, 'max': 8}\n",
            "  load.min1: {'mean': np.float64(1.1816129094552892), 'median': np.float64(1.11), 'std': 0.2578564770884285, 'min': 0.85, 'max': 3.12}\n",
            "  load.min5: {'mean': np.float64(1.171010554752106), 'median': np.float64(1.11), 'std': 0.23791446583471212, 'min': 1.0, 'max': 2.4}\n",
            "  load.min15: {'mean': np.float64(1.1431987829527557), 'median': np.float64(1.09), 'std': 0.22362676315970648, 'min': 0.46, 'max': 2.26}\n",
            "Stats for node 123:\n",
            "  cpu.user: {'mean': np.float64(11.182686959464581), 'median': np.float64(11.2), 'std': 1.1471436733000968, 'min': 3.0, 'max': 40.6}\n",
            "  mem.used: {'mean': np.float64(2289048714.872157), 'median': np.float64(2285285376.0), 'std': 119493301.99306016, 'min': 1776345088, 'max': 3938975744}\n",
            "  load.cpucore: {'mean': np.float64(8.0), 'median': np.float64(8.0), 'std': 0.0, 'min': 8, 'max': 8}\n",
            "  load.min1: {'mean': np.float64(1.1732099516700882), 'median': np.float64(1.09), 'std': 0.31505415108898527, 'min': 0.87, 'max': 3.94}\n",
            "  load.min5: {'mean': np.float64(1.162276705025264), 'median': np.float64(1.09), 'std': 0.2943921724657739, 'min': 0.99, 'max': 2.63}\n",
            "  load.min15: {'mean': np.float64(1.1327028132129513), 'median': np.float64(1.08), 'std': 0.271490134124568, 'min': 0.58, 'max': 2.43}\n",
            "Stats for node 124:\n",
            "  cpu.user: {'mean': np.float64(11.149670997935853), 'median': np.float64(11.2), 'std': 1.031669996236292, 'min': 3.0, 'max': 29.2}\n",
            "  mem.used: {'mean': np.float64(2312238284.2113366), 'median': np.float64(2318544896.0), 'std': 51519209.566430345, 'min': 1797902336, 'max': 2983526400}\n",
            "  load.cpucore: {'mean': np.float64(8.0), 'median': np.float64(8.0), 'std': 0.0, 'min': 8, 'max': 8}\n",
            "  load.min1: {'mean': np.float64(1.153352972138879), 'median': np.float64(1.09), 'std': 0.23982321320068947, 'min': 0.92, 'max': 3.16}\n",
            "  load.min5: {'mean': np.float64(1.1432896923277331), 'median': np.float64(1.09), 'std': 0.22075362241513014, 'min': 1.0, 'max': 2.37}\n",
            "  load.min15: {'mean': np.float64(1.1175380499651033), 'median': np.float64(1.08), 'std': 0.2030651616862042, 'min': 0.63, 'max': 2.14}\n"
          ]
        }
      ],
      "source": [
        "metrics_path = \"/content/concurrent data/metrics\"\n",
        "\n",
        "for nodeID in [113, 117, 122, 123, 124]:\n",
        "    csv_file = os.path.join(metrics_path, f\"wally{nodeID}_metrics_concurrent.csv\")\n",
        "    metricData = readMetrics(nodeID, csv_file)\n",
        "    plotFeaturesDistributionsMetrics(nodeID, metricData)\n",
        "\n",
        "    stats = caluclateStatisticalProperties(metricData)\n",
        "    print(f\"Stats for node {nodeID}:\")\n",
        "    for k, v in stats.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xb_AXA0xP9XM",
        "outputId": "ce68e721-2593-4200-e226-fadeefd08c56"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_8d1cfc3e-b5a3-4931-a092-f26ca2fef3fa\", \"Histograms.zip\", 69679)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download output PDFs locally\n",
        "# Create a folder to store all output PDFs\n",
        "output_folder = \"Histograms\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Move all generated PDFs into this folder\n",
        "for nodeID in [113, 117, 122, 123, 124]:\n",
        "    pdf_name = f\"{nodeID}_new.pdf\"\n",
        "    if os.path.exists(pdf_name):\n",
        "        shutil.move(pdf_name, os.path.join(output_folder, pdf_name))\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(output_folder, 'zip', output_folder)\n",
        "\n",
        "# Download the zip to this local machine\n",
        "files.download(f\"{output_folder}.zip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjtw5cqUJL8h"
      },
      "source": [
        "## investigateTraces.py for sequential data\n",
        "\n",
        "Note: the script trace_to_csv.py was written before pandas 2.0, where DataFrame.append() was removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtORcOhbo518",
        "outputId": "b9ded488-d7da-400d-8225-367c13caeabd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.3.5\n",
            "  Downloading pandas-1.3.5.tar.gz (4.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from pandas==1.3.5) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.12/dist-packages (from pandas==1.3.5) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.12/dist-packages (from pandas==1.3.5) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.17.0)\n",
            "Building wheels for collected packages: pandas\n",
            "  Building wheel for pandas (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas: filename=pandas-1.3.5-cp312-cp312-linux_x86_64.whl size=40409644 sha256=d4f476bd7d3287b432cde6caad32c80931309d77cf7abe6cdd9e1d62005b6999\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/15/7f/43571d4c48966d63f4f7a640e9345b57e23f1656d4d5b81b16\n",
            "Successfully built pandas\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.3.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires pandas>=1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.3.5 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.3.5 which is incompatible.\n",
            "libpysal 4.13.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.3.5 which is incompatible.\n",
            "xarray 2025.9.0 requires pandas>=2.2, but you have pandas 1.3.5 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.3.5 which is incompatible.\n",
            "bigframes 2.18.0 requires pandas>=1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "statsmodels 0.14.5 requires pandas!=2.1.0,>=1.4, but you have pandas 1.3.5 which is incompatible.\n",
            "arviz 0.22.0 requires pandas>=2.1.0, but you have pandas 1.3.5 which is incompatible.\n",
            "geopandas 1.1.1 requires pandas>=2.0.0, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.3.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas==1.3.5\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env PYTHON_VERSION=3.10\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y python3.10 python3.10-distutils\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 2\n",
        "!sudo update-alternatives --config python3\n",
        "!python3 -m pip install --upgrade pip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA7zl0f5GfCw",
        "outputId": "306e9745-0305-48f4-8754-6b7b6eaff2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHON_VERSION=3.10\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,006 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,795 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,266 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,273 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
            "Fetched 24.3 MB in 4s (6,664 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'python3-distutils' instead of 'python3.10-distutils'\n",
            "python3-distutils is already the newest version (3.10.8-1~22.04).\n",
            "python3-distutils set to manually installed.\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.11).\n",
            "python3.10 set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "There are 2 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                 Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.12   2         auto mode\n",
            "  1            /usr/bin/python3.10   2         manual mode\n",
            "  2            /usr/bin/python3.12   2         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: \n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 2\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1\n",
        "!sudo update-alternatives --set python3 /usr/bin/python3.10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuApN3z3G_5P",
        "outputId": "f9c37aad-ce1b-4f03-a2f3-aacf5ef0d38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: using /usr/bin/python3.10 to provide /usr/bin/python3 (python3) in auto mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddxfrPODHCl5",
        "outputId": "88d7f465-1523-44cf-be65-86ccfe056192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -sS https://bootstrap.pypa.io/get-pip.py | python3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxy65e64G2H9",
        "outputId": "f36bebfe-3bdc-4aa9-ac96-9ee3ff9f17bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pip\n",
            "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wheel\n",
            "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [pip]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pip-25.2 setuptools-80.9.0 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --upgrade pip\n",
        "!python3 -m pip install numpy==1.21.6 pandas==1.3.5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl7r5zH0HJVa",
        "outputId": "d4091d87-f1eb-4a97-9567-0c53661ded8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (25.2)\n",
            "Collecting numpy==1.21.6\n",
            "  Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting pandas==1.3.5\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting python-dateutil>=2.7.3 (from pandas==1.3.5)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2017.3 (from pandas==1.3.5)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.16.0)\n",
            "Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Installing collected packages: pytz, python-dateutil, numpy, pandas\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4/4\u001b[0m [pandas]\n",
            "\u001b[1A\u001b[2KSuccessfully installed numpy-1.21.6 pandas-1.3.5 python-dateutil-2.9.0.post0 pytz-2025.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install seaborn==0.11.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "783zvgPwHeiq",
        "outputId": "5606c139-f6fa-42b3-ea10-d6d0de18365c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seaborn==0.11.2\n",
            "  Downloading seaborn-0.11.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from seaborn==0.11.2) (1.21.6)\n",
            "Collecting scipy>=1.0 (from seaborn==0.11.2)\n",
            "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.10/dist-packages (from seaborn==0.11.2) (1.3.5)\n",
            "Collecting matplotlib>=2.2 (from seaborn==0.11.2)\n",
            "  Downloading matplotlib-3.10.6-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib>=2.2->seaborn==0.11.2)\n",
            "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib>=2.2->seaborn==0.11.2)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib>=2.2->seaborn==0.11.2)\n",
            "  Downloading fonttools-4.59.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (109 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib>=2.2->seaborn==0.11.2)\n",
            "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting numpy>=1.15 (from seaborn==0.11.2)\n",
            "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting packaging>=20.0 (from matplotlib>=2.2->seaborn==0.11.2)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pillow>=8 (from matplotlib>=2.2->seaborn==0.11.2)\n",
            "  Downloading pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=2.2->seaborn==0.11.2) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->seaborn==0.11.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->seaborn==0.11.2) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn==0.11.2) (1.16.0)\n",
            "Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
            "Downloading matplotlib-3.10.6-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.59.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Downloading pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow, packaging, numpy, kiwisolver, fonttools, cycler, scipy, contourpy, matplotlib, seaborn\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 1.21.6\n",
            "\u001b[2K    Uninstalling numpy-1.21.6:\n",
            "\u001b[2K      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/10\u001b[0m [seaborn]\n",
            "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.59.2 kiwisolver-1.4.9 matplotlib-3.10.6 numpy-2.2.6 packaging-25.0 pillow-11.3.0 scipy-1.15.3 seaborn-0.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VArxEe02ciP8",
        "outputId": "eabfb2b0-2157-4afb-e538-0d27c3fdfdba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/multi-source-observability-dataset/workloads/boot_delete_config/trace_to_csv.py\", line 1, in <module>\n",
            "    import pandas as pd\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/__init__.py\", line 22, in <module>\n",
            "    from pandas.compat import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/compat/__init__.py\", line 15, in <module>\n",
            "    from pandas.compat.numpy import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/compat/numpy/__init__.py\", line 7, in <module>\n",
            "    from pandas.util.version import Version\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/__init__.py\", line 1, in <module>\n",
            "    from pandas.util._decorators import (  # noqa\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 14, in <module>\n",
            "    from pandas._libs.properties import cache_readonly  # noqa\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/_libs/__init__.py\", line 13, in <module>\n",
            "    from pandas._libs.interval import Interval\n",
            "  File \"pandas/_libs/interval.pyx\", line 1, in init pandas._libs.interval\n",
            "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/multi-source-observability-dataset/workloads/boot_delete_config/trace_to_csv.py --path /content/sequential_data/traces/boot_delete --out /content/sequential_boot_delete.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run trace_to_csv.py\n",
        "## Use the sequential data"
      ],
      "metadata": {
        "id": "SGwGJMYYiY3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/multi-source-observability-dataset/workloads/boot_delete_config/trace_to_csv.py \\\n",
        "  -p /content/sequential_data/traces/boot_delete \\\n",
        "  -o /content/sequential_data/traces/boot_delete_traces.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXQ1Uk9GtFgN",
        "outputId": "eee29818-6307-4be6-aace-b0dfca8fc961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/multi-source-observability-dataset/workloads/boot_delete_config/trace_to_csv.py\", line 1, in <module>\n",
            "    import pandas as pd\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\", line 22, in <module>\n",
            "    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/compat/__init__.py\", line 18, in <module>\n",
            "    from pandas.compat.numpy import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/compat/numpy/__init__.py\", line 4, in <module>\n",
            "    from pandas.util.version import Version\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/util/__init__.py\", line 2, in <module>\n",
            "    from pandas.util._decorators import (  # noqa:F401\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/util/_decorators.py\", line 14, in <module>\n",
            "    from pandas._libs.properties import cache_readonly\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/_libs/__init__.py\", line 13, in <module>\n",
            "    from pandas._libs.interval import Interval\n",
            "  File \"pandas/_libs/interval.pyx\", line 1, in init pandas._libs.interval\n",
            "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_dirs = [\n",
        "    \"/content/sequential_data/traces\",\n",
        "    \"/content/concurrent data/traces\"\n",
        "]\n",
        "\n",
        "script = \"/content/multi-source-observability-dataset/workloads/boot_delete_config/trace_to_csv.py\"\n",
        "\n",
        "for base in base_dirs:\n",
        "    for sub in [\"boot_delete\", \"image_create_delete\", \"network_create_delete\"]:\n",
        "        input_path = os.path.join(base, sub)\n",
        "        output_file = os.path.join(base, f\"{sub}_traces.csv\")\n",
        "        cmd = f\"python {script} -p {input_path} -o {output_file}\"\n",
        "        print(f\"âš¡ Running: {cmd}\")\n",
        "        os.system(cmd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_YQFvQYifnS",
        "outputId": "ecd92a3b-8988-49c5-9485-8fff58cef97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš¡ Running: python /content/multi-source-observability-dataset/workloads/boot_delete_config/trace_to_csv.py -p /content/sequential_data/traces/boot_delete -o /content/sequential_data/traces/boot_delete_traces.csv\n",
            "âš¡ Running: python /content/multi-source-observability-dataset/workloads/boot_delete_config/trace_to_csv.py -p /content/sequential_data/traces/image_create_delete -o /content/sequential_data/traces/image_create_delete_traces.csv\n",
            "âš¡ Running: python /content/multi-source-observability-dataset/workloads/boot_delete_config/trace_to_csv.py -p /content/sequential_data/traces/network_create_delete -o /content/sequential_data/traces/network_create_delete_traces.csv\n",
            "âš¡ Running: python /content/multi-source-observability-dataset/workloads/boot_delete_config/trace_to_csv.py -p /content/concurrent data/traces/boot_delete -o /content/concurrent data/traces/boot_delete_traces.csv\n",
            "âš¡ Running: python /content/multi-source-observability-dataset/workloads/boot_delete_config/trace_to_csv.py -p /content/concurrent data/traces/image_create_delete -o /content/concurrent data/traces/image_create_delete_traces.csv\n",
            "âš¡ Running: python /content/multi-source-observability-dataset/workloads/boot_delete_config/trace_to_csv.py -p /content/concurrent data/traces/network_create_delete -o /content/concurrent data/traces/network_create_delete_traces.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Paths to your dataset folders\n",
        "datasets = {\n",
        "    \"Sequential\": \"sequential_data\",\n",
        "    \"Concurrent\": \"concurrent data\"\n",
        "}\n",
        "\n",
        "def preview_csv(path, n=5):\n",
        "    \"\"\"Safely preview the first n rows of a CSV file.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        print(f\"\\nğŸ“„ {path} ({df.shape[0]} rows, {df.shape[1]} cols)\")\n",
        "        print(df.head(n))\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Could not read {path}: {e}\")\n",
        "\n",
        "for dataset_name, base_path in datasets.items():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"ğŸ”¹ Previewing {dataset_name} Dataset\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Preview metrics\n",
        "    metrics_dir = os.path.join(base_path, \"metrics\")\n",
        "    for f in os.listdir(metrics_dir):\n",
        "        if f.endswith(\".csv\"):\n",
        "            preview_csv(os.path.join(metrics_dir, f))\n",
        "\n",
        "    # Preview logs (aggregated CSV only, skip archives)\n",
        "    logs_dir = os.path.join(base_path, \"logs\")\n",
        "    agg_logs = [f for f in os.listdir(logs_dir) if f.endswith(\".csv\")]\n",
        "    for f in agg_logs:\n",
        "        preview_csv(os.path.join(logs_dir, f))\n",
        "\n",
        "    # Preview traces (CSV after running trace_to_csv.py)\n",
        "    traces_dir = os.path.join(base_path, \"traces\")\n",
        "    trace_csvs = [f for f in os.listdir(traces_dir) if f.endswith(\"_traces.csv\")]\n",
        "    for f in trace_csvs:\n",
        "        preview_csv(os.path.join(traces_dir, f))\n"
      ],
      "metadata": {
        "id": "h6Wy5IVMkL0S",
        "outputId": "1eb19f15-65f6-4773-db6c-138128a46867",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ”¹ Previewing Sequential Dataset\n",
            "================================================================================\n",
            "\n",
            "ğŸ“„ sequential_data/metrics/wally124_metrics.csv (270304 rows, 7 cols)\n",
            "                        now  cpu.user    mem.used  load.cpucore  load.min1  \\\n",
            "0  2019-11-19 16:43:28 CEST       9.9  1888342016             8        0.4   \n",
            "1  2019-11-19 16:43:28 CEST      11.5  1889329152             8        0.4   \n",
            "2  2019-11-19 16:43:28 CEST      13.0  1889071104             8        0.4   \n",
            "3  2019-11-19 16:43:28 CEST      18.0  1894617088             8        0.4   \n",
            "4  2019-11-19 16:43:29 CEST      18.4  1895440384             8        0.4   \n",
            "\n",
            "   load.min5  load.min15  \n",
            "0       1.04        1.31  \n",
            "1       1.04        1.31  \n",
            "2       1.04        1.31  \n",
            "3       1.04        1.31  \n",
            "4       1.04        1.31  \n",
            "\n",
            "ğŸ“„ sequential_data/metrics/wally122_metrics.csv (270771 rows, 7 cols)\n",
            "                        now  cpu.user    mem.used  load.cpucore  load.min1  \\\n",
            "0  2019-11-19 16:36:51 CEST      12.4  1973506048             8       0.22   \n",
            "1  2019-11-19 16:36:51 CEST      14.3  1973768192             8       0.22   \n",
            "2  2019-11-19 16:36:52 CEST      19.7  1980469248             8       0.22   \n",
            "3  2019-11-19 16:36:52 CEST      10.6  1981747200             8       0.22   \n",
            "4  2019-11-19 16:36:52 CEST      12.8  1981124608             8       0.22   \n",
            "\n",
            "   load.min5  load.min15  \n",
            "0       0.91        1.21  \n",
            "1       0.91        1.21  \n",
            "2       0.91        1.21  \n",
            "3       0.91        1.21  \n",
            "4       0.91        1.21  \n",
            "\n",
            "ğŸ“„ sequential_data/metrics/wally123_metrics.csv (247411 rows, 7 cols)\n",
            "                        now  cpu.user    mem.used  load.cpucore  load.min1  \\\n",
            "0  2019-11-19 16:46:17 CEST      20.1  2383872000             8       1.59   \n",
            "1  2019-11-19 16:46:17 CEST      25.6  2383962112             8       1.59   \n",
            "2  2019-11-19 16:46:18 CEST      23.4  2384277504             8       1.59   \n",
            "3  2019-11-19 16:46:18 CEST      22.8  2383347712             8       1.59   \n",
            "4  2019-11-19 16:46:18 CEST      31.9  2382991360             8       1.59   \n",
            "\n",
            "   load.min5  load.min15  \n",
            "0       1.57        1.49  \n",
            "1       1.57        1.49  \n",
            "2       1.57        1.49  \n",
            "3       1.57        1.49  \n",
            "4       1.57        1.49  \n",
            "\n",
            "ğŸ“„ sequential_data/metrics/wally113_metrics.csv (108900 rows, 7 cols)\n",
            "                        now  cpu.user     mem.used  load.cpucore  load.min1  \\\n",
            "0  2019-11-19 16:56:32 CEST      11.5  10221035520             8        0.8   \n",
            "1  2019-11-19 16:56:32 CEST      10.4  10221117440             8        0.8   \n",
            "2  2019-11-19 16:56:33 CEST      11.1  10222948352             8        0.8   \n",
            "3  2019-11-19 16:56:33 CEST      14.3  10223144960             8        0.8   \n",
            "4  2019-11-19 16:56:34 CEST      10.7  10222866432             8        0.8   \n",
            "\n",
            "   load.min5  load.min15  \n",
            "0       1.02        1.18  \n",
            "1       1.02        1.18  \n",
            "2       1.02        1.18  \n",
            "3       1.02        1.18  \n",
            "4       1.02        1.18  \n",
            "\n",
            "ğŸ“„ sequential_data/metrics/wally117_metrics.csv (298251 rows, 7 cols)\n",
            "                        now  cpu.user    mem.used  load.cpucore  load.min1  \\\n",
            "0  2019-11-19 16:33:05 CEST      16.1  2917302272             8       0.31   \n",
            "1  2019-11-19 16:33:05 CEST      23.2  2925170688             8       0.31   \n",
            "2  2019-11-19 16:33:06 CEST      10.9  2923724800             8       0.31   \n",
            "3  2019-11-19 16:33:06 CEST      11.6  2924593152             8       0.31   \n",
            "4  2019-11-19 16:33:06 CEST      11.7  2924658688             8       0.31   \n",
            "\n",
            "   load.min5  load.min15  \n",
            "0       0.82        1.22  \n",
            "1       0.82        1.22  \n",
            "2       0.82        1.22  \n",
            "3       0.82        1.22  \n",
            "4       0.82        1.22  \n",
            "\n",
            "ğŸ“„ sequential_data/logs/logs_aggregated_sequential.csv (139799 rows, 23 cols)\n",
            "                    _id           _index  _score    _type  Hostname  \\\n",
            "0  AW6EZkrrjbdRMJxglOB7  flog-2019.11.19     1.0  fluentd  wally113   \n",
            "1  AW6EZkrrjbdRMJxglOB8  flog-2019.11.19     1.0  fluentd  wally113   \n",
            "2  AW6EZkrrjbdRMJxglOB9  flog-2019.11.19     1.0  fluentd  wally113   \n",
            "3  AW6EZz91jbdRMJxglOCO  flog-2019.11.19     1.0  fluentd  wally113   \n",
            "4  AW6EZz91jbdRMJxglOCR  flog-2019.11.19     1.0  fluentd  wally113   \n",
            "\n",
            "                            user_id project_domain                Timestamp  \\\n",
            "0  7ea5c98d216a4d7580bc4e2499fda0d3        default  2019-11-19 17:00:48.274   \n",
            "1  7ea5c98d216a4d7580bc4e2499fda0d3        default  2019-11-19 17:01:50.119   \n",
            "2  7ea5c98d216a4d7580bc4e2499fda0d3        default  2019-11-19 17:01:50.210   \n",
            "3  7ea5c98d216a4d7580bc4e2499fda0d3        default  2019-11-19 17:01:50.317   \n",
            "4  7ea5c98d216a4d7580bc4e2499fda0d3        default  2019-11-19 17:02:52.398   \n",
            "\n",
            "                            @timestamp log_level  ...     programname  \\\n",
            "0  2019-11-19T17:00:48.274000000+01:00      INFO  ...  neutron-server   \n",
            "1  2019-11-19T17:01:50.119000000+01:00      INFO  ...  neutron-server   \n",
            "2  2019-11-19T17:01:50.210000000+01:00      INFO  ...  neutron-server   \n",
            "3  2019-11-19T17:01:50.317000000+01:00      INFO  ...  neutron-server   \n",
            "4  2019-11-19T17:02:52.398000000+01:00      INFO  ...  neutron-server   \n",
            "\n",
            "                             request_id python_module             Logger  \\\n",
            "0  aee61273-b63d-4287-8ed3-d63e9fcd4bcf  neutron.wsgi  openstack.neutron   \n",
            "1  98761699-6660-44ad-a5aa-18dc02b00977  neutron.wsgi  openstack.neutron   \n",
            "2  0da09d75-3f2b-44e1-8ab6-b2eddd84ffed  neutron.wsgi  openstack.neutron   \n",
            "3  9f21fe28-2976-4252-9dca-5e18f0c6180b  neutron.wsgi  openstack.neutron   \n",
            "4  a0a9499e-7c75-4b67-bfea-bd0986d3f410  neutron.wsgi  openstack.neutron   \n",
            "\n",
            "  user_domain domain_id http_status http_method http_version  http_url  \n",
            "0     default         -         NaN         NaN          NaN       NaN  \n",
            "1     default         -         NaN         NaN          NaN       NaN  \n",
            "2     default         -         NaN         NaN          NaN       NaN  \n",
            "3     default         -         NaN         NaN          NaN       NaN  \n",
            "4     default         -         NaN         NaN          NaN       NaN  \n",
            "\n",
            "[5 rows x 23 columns]\n",
            "\n",
            "================================================================================\n",
            "ğŸ”¹ Previewing Concurrent Dataset\n",
            "================================================================================\n",
            "\n",
            "ğŸ“„ concurrent data/metrics/wally117_metrics_concurrent.csv (1082190 rows, 7 cols)\n",
            "                        now  cpu.user    mem.used  load.cpucore  load.min1  \\\n",
            "0  2019-11-25 15:58:30 CEST       8.0  1875214336             8       1.43   \n",
            "1  2019-11-25 15:58:31 CEST       8.0  1875156992             8       1.43   \n",
            "2  2019-11-25 15:58:31 CEST       6.0  1875480576             8       1.39   \n",
            "3  2019-11-25 15:58:31 CEST       6.0  1875570688             8       1.39   \n",
            "4  2019-11-25 15:58:31 CEST       7.0  1875369984             8       1.39   \n",
            "\n",
            "   load.min5  load.min15  \n",
            "0       1.38        0.63  \n",
            "1       1.38        0.63  \n",
            "2       1.37        0.64  \n",
            "3       1.37        0.64  \n",
            "4       1.37        0.64  \n",
            "\n",
            "ğŸ“„ concurrent data/metrics/wally122_metrics_concurrent.csv (932092 rows, 7 cols)\n",
            "                        now  cpu.user    mem.used  load.cpucore  load.min1  \\\n",
            "0  2019-11-25 15:57:38 CEST       7.0  1854873600             8       1.42   \n",
            "1  2019-11-25 15:57:38 CEST       6.0  1855209472             8       1.42   \n",
            "2  2019-11-25 15:57:38 CEST       7.0  1855492096             8       1.42   \n",
            "3  2019-11-25 15:57:38 CEST       7.0  1855131648             8       1.42   \n",
            "4  2019-11-25 15:57:38 CEST       6.0  1855553536             8       1.42   \n",
            "\n",
            "   load.min5  load.min15  \n",
            "0        1.1        0.46  \n",
            "1        1.1        0.46  \n",
            "2        1.1        0.46  \n",
            "3        1.1        0.46  \n",
            "4        1.1        0.46  \n",
            "\n",
            "ğŸ“„ concurrent data/metrics/wally124_metrics_concurrent.csv (944219 rows, 7 cols)\n",
            "                        now  cpu.user    mem.used  load.cpucore  load.min1  \\\n",
            "0  2019-11-25 15:57:09 CEST       5.0  1800339456             8       3.03   \n",
            "1  2019-11-25 15:57:09 CEST       8.0  1800421376             8       3.03   \n",
            "2  2019-11-25 15:57:09 CEST       8.0  1800888320             8       3.03   \n",
            "3  2019-11-25 15:57:09 CEST       7.0  1800646656             8       3.03   \n",
            "4  2019-11-25 15:57:10 CEST       5.0  1801289728             8       3.03   \n",
            "\n",
            "   load.min5  load.min15  \n",
            "0       1.65        0.63  \n",
            "1       1.65        0.63  \n",
            "2       1.65        0.63  \n",
            "3       1.65        0.63  \n",
            "4       1.65        0.63  \n",
            "\n",
            "ğŸ“„ concurrent data/metrics/wally113_metrics_concurrent.csv (406860 rows, 7 cols)\n",
            "                        now  cpu.user     mem.used  load.cpucore  load.min1  \\\n",
            "0  2019-11-25 15:58:58 CEST      15.5  11359891456             8       1.99   \n",
            "1  2019-11-25 15:58:58 CEST      14.0  11360206848             8       1.99   \n",
            "2  2019-11-25 15:58:58 CEST      14.9  11360346112             8       1.99   \n",
            "3  2019-11-25 15:58:59 CEST      16.1  11360571392             8       1.99   \n",
            "4  2019-11-25 15:58:59 CEST      16.4  11361447936             8       1.99   \n",
            "\n",
            "   load.min5  load.min15  \n",
            "0       1.93        2.11  \n",
            "1       1.93        2.11  \n",
            "2       1.93        2.11  \n",
            "3       1.93        2.11  \n",
            "4       1.93        2.11  \n",
            "\n",
            "ğŸ“„ concurrent data/metrics/wally123_metrics_concurrent.csv (936687 rows, 7 cols)\n",
            "                        now  cpu.user    mem.used  load.cpucore  load.min1  \\\n",
            "0  2019-11-25 15:59:28 CEST       8.0  1777172480             8       2.75   \n",
            "1  2019-11-25 15:59:28 CEST       7.0  1777205248             8       2.75   \n",
            "2  2019-11-25 15:59:28 CEST       7.0  1777639424             8       2.75   \n",
            "3  2019-11-25 15:59:28 CEST       7.0  1777573888             8       2.75   \n",
            "4  2019-11-25 15:59:28 CEST       6.0  1777893376             8       2.75   \n",
            "\n",
            "   load.min5  load.min15  \n",
            "0       1.49        0.58  \n",
            "1       1.49        0.58  \n",
            "2       1.49        0.58  \n",
            "3       1.49        0.58  \n",
            "4       1.49        0.58  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-484780241.py:13: DtypeWarning: Columns (23,24,25,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“„ concurrent data/logs/logs_aggregated_concurrent.csv (169230 rows, 30 cols)\n",
            "                    _id           _index  _score    _type  Hostname  \\\n",
            "0  AW6i3n4JtOpH-3YEBpA7  flog-2019.11.25     1.0  fluentd  wally113   \n",
            "1  AW6i3n4JtOpH-3YEBpBB  flog-2019.11.25     1.0  fluentd  wally113   \n",
            "2  AW6i3n4JtOpH-3YEBpBD  flog-2019.11.25     1.0  fluentd  wally113   \n",
            "3  AW6i3n4JtOpH-3YEBpBL  flog-2019.11.25     1.0  fluentd  wally113   \n",
            "4  AW6i3n4JtOpH-3YEBpBR  flog-2019.11.25     1.0  fluentd  wally113   \n",
            "\n",
            "                            user_id project_domain                Timestamp  \\\n",
            "0  2ac494fefe7848abb12faa65b74cd990              -  2019-11-25 15:01:49.683   \n",
            "1  2537c5418d8a45d6abfcade96cfd838a              -  2019-11-25 15:01:49.877   \n",
            "2  2537c5418d8a45d6abfcade96cfd838a        default  2019-11-25 15:01:52.744   \n",
            "3  2ac494fefe7848abb12faa65b74cd990        default  2019-11-25 15:01:48.510   \n",
            "4                               NaN            NaN  2019-11-25 15:01:56.520   \n",
            "\n",
            "                            @timestamp log_level  ...  http_method  \\\n",
            "0  2019-11-25T15:01:49.683000000+01:00      INFO  ...          NaN   \n",
            "1  2019-11-25T15:01:49.877000000+01:00      INFO  ...          NaN   \n",
            "2  2019-11-25T15:01:52.744000000+01:00   WARNING  ...          NaN   \n",
            "3  2019-11-25T15:01:48.510000000+01:00      INFO  ...          NaN   \n",
            "4  2019-11-25T15:01:56.520000000+01:00      INFO  ...          NaN   \n",
            "\n",
            "  http_version http_url chunk next_retry_seconds error retry_time message  \\\n",
            "0          NaN      NaN   NaN                NaN   NaN        NaN     NaN   \n",
            "1          NaN      NaN   NaN                NaN   NaN        NaN     NaN   \n",
            "2          NaN      NaN   NaN                NaN   NaN        NaN     NaN   \n",
            "3          NaN      NaN   NaN                NaN   NaN        NaN     NaN   \n",
            "4          NaN      NaN   NaN                NaN   NaN        NaN     NaN   \n",
            "\n",
            "  chunk_id  worker  \n",
            "0      NaN     NaN  \n",
            "1      NaN     NaN  \n",
            "2      NaN     NaN  \n",
            "3      NaN     NaN  \n",
            "4      NaN     NaN  \n",
            "\n",
            "[5 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Investigate the Reports data"
      ],
      "metadata": {
        "id": "l63eF45Zii6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/concurrent data/reports/output_boot.json\") as f:\n",
        "    boot_report = json.load(f)\n",
        "\n",
        "print(boot_report.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgmutpOXinY5",
        "outputId": "eb950e63-87fb-4408-9828-f4dbfceddcca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['info', 'tasks'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Paths to report folders\n",
        "report_dirs = [\n",
        "    \"/content/sequential_data/reports\",\n",
        "    \"/content/concurrent data/reports\"\n",
        "]\n",
        "\n",
        "for report_dir in report_dirs:\n",
        "    print(f\"\\n=== Reports in {report_dir} ===\")\n",
        "\n",
        "    for fname in sorted(os.listdir(report_dir)):\n",
        "        if fname.endswith(\".json\"):  # only JSON reports\n",
        "            fpath = os.path.join(report_dir, fname)\n",
        "            print(f\"\\n--- {fname} ---\")\n",
        "\n",
        "            with open(fpath, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Print nicely formatted JSON (first 500 chars if too big)\n",
        "            pretty = json.dumps(data, indent=2)[:500]\n",
        "            print(pretty + (\"...\" if len(pretty) == 500 else \"\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIfpO2XcqtPL",
        "outputId": "1435ae00-10de-41cf-9e75-aaae71af2c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Reports in /content/sequential_data/reports ===\n",
            "\n",
            "--- j_boot_delete_report.json ---\n",
            "{\n",
            "  \"info\": {\n",
            "    \"generated_at\": \"2019-11-21T09:22:47\",\n",
            "    \"rally_version\": \"1.5.1\",\n",
            "    \"format_version\": \"1.2\"\n",
            "  },\n",
            "  \"tasks\": [\n",
            "    {\n",
            "      \"uuid\": \"67d2d0fd-c68d-4a9c-b315-1e9c608229c1\",\n",
            "      \"title\": \"\",\n",
            "      \"description\": \"\",\n",
            "      \"status\": \"finished\",\n",
            "      \"tags\": [],\n",
            "      \"env_uuid\": \"aeade984-8d0c-48d4-9dca-a8e1a1402b50\",\n",
            "      \"env_name\": \"ajay_test\",\n",
            "      \"created_at\": \"2019-11-19T21:45:41\",\n",
            "      \"updated_at\": \"2019-11-20T01:08:43\",\n",
            "      \"pass_sla\": true,\n",
            "      \"subtasks\": ...\n",
            "\n",
            "--- j_image_report.json ---\n",
            "{\n",
            "  \"info\": {\n",
            "    \"generated_at\": \"2019-11-21T08:53:59\",\n",
            "    \"rally_version\": \"1.5.1\",\n",
            "    \"format_version\": \"1.2\"\n",
            "  },\n",
            "  \"tasks\": [\n",
            "    {\n",
            "      \"uuid\": \"ed73877f-0eeb-47fe-8ce7-eb79db602a6b\",\n",
            "      \"title\": \"\",\n",
            "      \"description\": \"\",\n",
            "      \"status\": \"finished\",\n",
            "      \"tags\": [],\n",
            "      \"env_uuid\": \"aeade984-8d0c-48d4-9dca-a8e1a1402b50\",\n",
            "      \"env_name\": \"ajay_test\",\n",
            "      \"created_at\": \"2019-11-19T17:38:39\",\n",
            "      \"updated_at\": \"2019-11-19T18:44:18\",\n",
            "      \"pass_sla\": true,\n",
            "      \"subtasks\": ...\n",
            "\n",
            "--- j_network_output.json ---\n",
            "{\n",
            "  \"info\": {\n",
            "    \"generated_at\": \"2019-11-19T21:42:06\",\n",
            "    \"rally_version\": \"1.5.1\",\n",
            "    \"format_version\": \"1.2\"\n",
            "  },\n",
            "  \"tasks\": [\n",
            "    {\n",
            "      \"uuid\": \"283053d8-7c93-465e-905d-d81969695301\",\n",
            "      \"title\": \"\",\n",
            "      \"description\": \"\",\n",
            "      \"status\": \"finished\",\n",
            "      \"tags\": [],\n",
            "      \"env_uuid\": \"aeade984-8d0c-48d4-9dca-a8e1a1402b50\",\n",
            "      \"env_name\": \"ajay_test\",\n",
            "      \"created_at\": \"2019-11-19T19:59:42\",\n",
            "      \"updated_at\": \"2019-11-19T20:27:26\",\n",
            "      \"pass_sla\": true,\n",
            "      \"subtasks\": ...\n",
            "\n",
            "=== Reports in /content/concurrent data/reports ===\n",
            "\n",
            "--- output_boot.json ---\n",
            "{\n",
            "  \"info\": {\n",
            "    \"generated_at\": \"2019-11-25T19:36:40\",\n",
            "    \"rally_version\": \"1.5.1\",\n",
            "    \"format_version\": \"1.2\"\n",
            "  },\n",
            "  \"tasks\": [\n",
            "    {\n",
            "      \"uuid\": \"a1749852-1385-4f64-ac20-89df1f0677ca\",\n",
            "      \"title\": \"\",\n",
            "      \"description\": \"\",\n",
            "      \"status\": \"finished\",\n",
            "      \"tags\": [],\n",
            "      \"env_uuid\": \"aeade984-8d0c-48d4-9dca-a8e1a1402b50\",\n",
            "      \"env_name\": \"ajay_test\",\n",
            "      \"created_at\": \"2019-11-25T15:12:13\",\n",
            "      \"updated_at\": \"2019-11-25T19:36:19\",\n",
            "      \"pass_sla\": true,\n",
            "      \"subtasks\": ...\n",
            "\n",
            "--- output_image.json ---\n",
            "{\n",
            "  \"info\": {\n",
            "    \"generated_at\": \"2019-11-25T18:59:49\",\n",
            "    \"rally_version\": \"1.5.1\",\n",
            "    \"format_version\": \"1.2\"\n",
            "  },\n",
            "  \"tasks\": [\n",
            "    {\n",
            "      \"uuid\": \"55300366-8457-402f-8570-4b7b46f4cbb5\",\n",
            "      \"title\": \"\",\n",
            "      \"description\": \"\",\n",
            "      \"status\": \"finished\",\n",
            "      \"tags\": [],\n",
            "      \"env_uuid\": \"aeade984-8d0c-48d4-9dca-a8e1a1402b50\",\n",
            "      \"env_name\": \"ajay_test\",\n",
            "      \"created_at\": \"2019-11-25T15:12:28\",\n",
            "      \"updated_at\": \"2019-11-25T17:51:08\",\n",
            "      \"pass_sla\": true,\n",
            "      \"subtasks\": ...\n",
            "\n",
            "--- output_network.json ---\n",
            "{\n",
            "  \"info\": {\n",
            "    \"generated_at\": \"2019-11-25T19:00:35\",\n",
            "    \"rally_version\": \"1.5.1\",\n",
            "    \"format_version\": \"1.2\"\n",
            "  },\n",
            "  \"tasks\": [\n",
            "    {\n",
            "      \"uuid\": \"94c9c5f0-fefd-4746-90df-8eaa96fed86d\",\n",
            "      \"title\": \"\",\n",
            "      \"description\": \"\",\n",
            "      \"status\": \"finished\",\n",
            "      \"tags\": [],\n",
            "      \"env_uuid\": \"aeade984-8d0c-48d4-9dca-a8e1a1402b50\",\n",
            "      \"env_name\": \"ajay_test\",\n",
            "      \"created_at\": \"2019-11-25T15:12:53\",\n",
            "      \"updated_at\": \"2019-11-25T18:18:40\",\n",
            "      \"pass_sla\": true,\n",
            "      \"subtasks\": ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def flatten_reports(dir_path, dataset_name):\n",
        "    \"\"\"\n",
        "    Flatten JSON reports in a directory into a DataFrame.\n",
        "\n",
        "    :param dir_path: Path to the reports folder\n",
        "    :param dataset_name: Label for dataset (\"sequential\" or \"concurrent\")\n",
        "    :return: pandas DataFrame\n",
        "    \"\"\"\n",
        "    all_tasks = []\n",
        "\n",
        "    for file_name in os.listdir(dir_path):\n",
        "        if file_name.endswith(\".json\"):\n",
        "            file_path = os.path.join(dir_path, file_name)\n",
        "            with open(file_path, \"r\") as f:\n",
        "                report = json.load(f)\n",
        "            for task in report.get(\"tasks\", []):\n",
        "                flattened = {\n",
        "                    \"dataset\": dataset_name,\n",
        "                    \"report_file\": file_name,\n",
        "                    \"task_uuid\": task.get(\"uuid\"),\n",
        "                    \"title\": task.get(\"title\"),\n",
        "                    \"description\": task.get(\"description\"),\n",
        "                    \"status\": task.get(\"status\"),\n",
        "                    \"env_uuid\": task.get(\"env_uuid\"),\n",
        "                    \"env_name\": task.get(\"env_name\"),\n",
        "                    \"created_at\": task.get(\"created_at\"),\n",
        "                    \"updated_at\": task.get(\"updated_at\"),\n",
        "                    \"pass_sla\": task.get(\"pass_sla\"),\n",
        "                    \"subtasks\": task.get(\"subtasks\")  # Keep nested for now\n",
        "                }\n",
        "                all_tasks.append(flattened)\n",
        "\n",
        "    df = pd.DataFrame(all_tasks)\n",
        "    # Optional: parse timestamps\n",
        "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
        "    df[\"updated_at\"] = pd.to_datetime(df[\"updated_at\"])\n",
        "    return df\n",
        "\n",
        "# Sequential reports\n",
        "seq_reports_dir = \"/content/sequential_data/reports\"\n",
        "df_sequential = flatten_reports(seq_reports_dir, \"sequential\")\n",
        "df_sequential.to_csv(\"/content/sequential_reports_flattened.csv\", index=False)\n",
        "print(\"Sequential reports flattened:\", df_sequential.shape)\n",
        "\n",
        "# Concurrent reports\n",
        "concurrent_reports_dir = \"/content/concurrent data/reports\"\n",
        "df_concurrent = flatten_reports(concurrent_reports_dir, \"concurrent\")\n",
        "df_concurrent.to_csv(\"/content/concurrent_reports_flattened.csv\", index=False)\n",
        "print(\"Concurrent reports flattened:\", df_concurrent.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpHhKwwvrP-T",
        "outputId": "43c31021-d697-4f24-8709-5e1330d2f020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential reports flattened: (3, 12)\n",
            "Concurrent reports flattened: (3, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def flatten_reports(report_dir, dataset_name):\n",
        "    \"\"\"\n",
        "    Flatten all JSON reports in a directory into a DataFrame.\n",
        "    Each subtask becomes a row, with parent task info preserved.\n",
        "    \"\"\"\n",
        "    report_files = Path(report_dir).glob(\"*.json\")\n",
        "    all_rows = []\n",
        "\n",
        "    for report_file in report_files:\n",
        "        with open(report_file) as f:\n",
        "            report = json.load(f)\n",
        "\n",
        "        for task in report[\"tasks\"]:\n",
        "            task_info = {k: task[k] for k in task if k != \"subtasks\"}\n",
        "            task_info[\"dataset\"] = dataset_name\n",
        "            task_info[\"report_file\"] = report_file.name\n",
        "\n",
        "            # If subtasks exist, flatten each subtask\n",
        "            subtasks = task.get(\"subtasks\", [])\n",
        "            if subtasks:\n",
        "                for sub in subtasks:\n",
        "                    row = task_info.copy()\n",
        "                    # Merge subtask info\n",
        "                    for k, v in sub.items():\n",
        "                        row[f\"subtask_{k}\"] = v\n",
        "                    all_rows.append(row)\n",
        "            else:\n",
        "                all_rows.append(task_info)\n",
        "\n",
        "    df = pd.DataFrame(all_rows)\n",
        "    return df\n",
        "\n",
        "# Paths to your reports directories\n",
        "sequential_reports_dir = \"/content/sequential_data/reports\"\n",
        "concurrent_reports_dir = \"/content/concurrent data/reports\"\n",
        "\n",
        "# Flatten both datasets\n",
        "df_sequential = flatten_reports(sequential_reports_dir, \"sequential\")\n",
        "df_concurrent = flatten_reports(concurrent_reports_dir, \"concurrent\")\n",
        "\n",
        "# Optional: view shapes\n",
        "print(\"Sequential flattened shape:\", df_sequential.shape)\n",
        "print(\"Concurrent flattened shape:\", df_concurrent.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXt5vtKoraV1",
        "outputId": "41e74f34-3c5f-4148-9693-2240406090d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential flattened shape: (3, 20)\n",
            "Concurrent flattened shape: (3, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def flatten_reports(report_dir, dataset_name):\n",
        "    \"\"\"\n",
        "    Flatten all JSON reports in a directory.\n",
        "\n",
        "    Args:\n",
        "        report_dir (str): Path to the report directory.\n",
        "        dataset_name (str): 'concurrent' or 'sequential'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Flattened DataFrame with one row per subtask.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    for file in os.listdir(report_dir):\n",
        "        if file.endswith(\".json\"):\n",
        "            path = os.path.join(report_dir, file)\n",
        "            with open(path, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            tasks = data.get(\"tasks\", [])\n",
        "            for task in tasks:\n",
        "                task_uuid = task.get(\"uuid\")\n",
        "                task_title = task.get(\"title\")\n",
        "                task_desc = task.get(\"description\")\n",
        "                task_status = task.get(\"status\")\n",
        "                task_tags = task.get(\"tags\", [])\n",
        "                env_uuid = task.get(\"env_uuid\")\n",
        "                env_name = task.get(\"env_name\")\n",
        "                created_at = task.get(\"created_at\")\n",
        "                updated_at = task.get(\"updated_at\")\n",
        "                pass_sla = task.get(\"pass_sla\")\n",
        "\n",
        "                # Flatten subtasks\n",
        "                subtasks = task.get(\"subtasks\", [])\n",
        "                if not subtasks:\n",
        "                    rows.append({\n",
        "                        \"uuid\": task_uuid,\n",
        "                        \"title\": task_title,\n",
        "                        \"description\": task_desc,\n",
        "                        \"status\": task_status,\n",
        "                        \"tags\": task_tags,\n",
        "                        \"env_uuid\": env_uuid,\n",
        "                        \"env_name\": env_name,\n",
        "                        \"created_at\": created_at,\n",
        "                        \"updated_at\": updated_at,\n",
        "                        \"pass_sla\": pass_sla,\n",
        "                        \"dataset\": dataset_name,\n",
        "                        \"report_file\": file,\n",
        "                        \"subtask_uuid\": None,\n",
        "                        \"subtask_title\": None,\n",
        "                        \"subtask_description\": None,\n",
        "                        \"subtask_status\": None,\n",
        "                        \"subtask_created_at\": None,\n",
        "                        \"subtask_updated_at\": None,\n",
        "                        \"subtask_sla\": None,\n",
        "                        \"subtask_workloads\": None\n",
        "                    })\n",
        "                else:\n",
        "                    for sub in subtasks:\n",
        "                        rows.append({\n",
        "                            \"uuid\": task_uuid,\n",
        "                            \"title\": task_title,\n",
        "                            \"description\": task_desc,\n",
        "                            \"status\": task_status,\n",
        "                            \"tags\": task_tags,\n",
        "                            \"env_uuid\": env_uuid,\n",
        "                            \"env_name\": env_name,\n",
        "                            \"created_at\": created_at,\n",
        "                            \"updated_at\": updated_at,\n",
        "                            \"pass_sla\": pass_sla,\n",
        "                            \"dataset\": dataset_name,\n",
        "                            \"report_file\": file,\n",
        "                            \"subtask_uuid\": sub.get(\"uuid\"),\n",
        "                            \"subtask_title\": sub.get(\"title\"),\n",
        "                            \"subtask_description\": sub.get(\"description\"),\n",
        "                            \"subtask_status\": sub.get(\"status\"),\n",
        "                            \"subtask_created_at\": sub.get(\"created_at\"),\n",
        "                            \"subtask_updated_at\": sub.get(\"updated_at\"),\n",
        "                            \"subtask_sla\": sub.get(\"pass_sla\"),\n",
        "                            \"subtask_workloads\": sub.get(\"workloads\")\n",
        "                        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n",
        "# Paths to your reports folders\n",
        "sequential_reports_dir = \"/content/sequential_data/reports\"\n",
        "concurrent_reports_dir = \"/content/concurrent data/reports\"\n",
        "\n",
        "# Flatten both datasets\n",
        "df_sequential = flatten_reports(sequential_reports_dir, \"sequential\")\n",
        "df_concurrent = flatten_reports(concurrent_reports_dir, \"concurrent\")\n",
        "\n",
        "# Optional: Save to CSV for later use\n",
        "df_sequential.to_csv(\"/content/flattened_sequential_reports.csv\", index=False)\n",
        "df_concurrent.to_csv(\"/content/flattened_concurrent_reports.csv\", index=False)\n",
        "\n",
        "print(\"Sequential flattened shape:\", df_sequential.shape)\n",
        "print(\"Concurrent flattened shape:\", df_concurrent.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1C6Fy1LutN4",
        "outputId": "108facc0-e270-4cd6-8769-a5da5a24ca21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential flattened shape: (3, 20)\n",
            "Concurrent flattened shape: (3, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine traces operations into a single dataset for both sequential and concurrent"
      ],
      "metadata": {
        "id": "MfS7y8bnB0hC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Files for sequential traces\n",
        "sequential_files = {\n",
        "    \"/content/traces.csv\": \"boot_delete\",\n",
        "    \"/content/image_create_delete_traces.csv\": \"image_create_delete\",\n",
        "    \"/content/network_create_delete_traces.csv\": \"network_create_delete\"\n",
        "}\n",
        "\n",
        "# Files for concurrent traces\n",
        "concurrent_files = {\n",
        "    \"/content/boot_delete_concurrent_traces.csv\": \"boot_delete\",\n",
        "    \"/content/image_create_delete_concurrent_traces.csv\": \"image_create_delete\",\n",
        "    \"/content/network_create_delete_concurrent_traces.csv\": \"network_create_delete\"\n",
        "}\n",
        "\n",
        "def combine_traces(files_dict):\n",
        "    df_list = []\n",
        "    for path, operation in files_dict.items():\n",
        "        df = pd.read_csv(path)\n",
        "        df[\"operation\"] = operation\n",
        "        df_list.append(df)\n",
        "    combined = pd.concat(df_list, ignore_index=True)\n",
        "    return combined\n",
        "\n",
        "# Combine sequential and concurrent separately\n",
        "combined_sequential = combine_traces(sequential_files)\n",
        "combined_concurrent = combine_traces(concurrent_files)\n",
        "\n",
        "# Optional: check shapes\n",
        "print(\"Sequential traces shape:\", combined_sequential.shape)\n",
        "print(\"Concurrent traces shape:\", combined_concurrent.shape)\n",
        "\n",
        "# Save to CSV\n",
        "combined_sequential.to_csv(\"/content/combined_sequential_traces.csv\", index=False)\n",
        "combined_concurrent.to_csv(\"/content/combined_concurrent_traces.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgOyMzlWB8Oj",
        "outputId": "6808b1e8-42ea-4fd6-bb6b-1289cdcd58c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential traces shape: (872396, 10)\n",
            "Concurrent traces shape: (889379, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine all metrics datasets into a single combined dataset for both soncurrent and sequential"
      ],
      "metadata": {
        "id": "O9deYW3GJVHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Paths for metrics\n",
        "sequential_metrics_dir = \"/content/sequential_data/metrics\"\n",
        "concurrent_metrics_dir = \"/content/concurrent data/metrics\"\n",
        "\n",
        "def combine_metrics(metrics_dir):\n",
        "    combined_list = []\n",
        "    for file in os.listdir(metrics_dir):\n",
        "        if file.endswith(\".csv\"):\n",
        "            path = os.path.join(metrics_dir, file)\n",
        "            df = pd.read_csv(path)\n",
        "            df[\"node\"] = file.replace(\".csv\", \"\")  # Add column for source node\n",
        "            combined_list.append(df)\n",
        "    combined_df = pd.concat(combined_list, ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "# Combine sequential metrics\n",
        "combined_sequential_metrics = combine_metrics(sequential_metrics_dir)\n",
        "print(\"Sequential metrics shape:\", combined_sequential_metrics.shape)\n",
        "combined_sequential_metrics.to_csv(\"/content/combined_sequential_metrics.csv\", index=False)\n",
        "\n",
        "# Combine concurrent metrics\n",
        "combined_concurrent_metrics = combine_metrics(concurrent_metrics_dir)\n",
        "print(\"Concurrent metrics shape:\", combined_concurrent_metrics.shape)\n",
        "combined_concurrent_metrics.to_csv(\"/content/combined concurrent_metrics.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqRion_rJcAv",
        "outputId": "db7f6bb0-6015-4acf-d659-ab72879704e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential metrics shape: (1195637, 8)\n",
            "Concurrent metrics shape: (4302048, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "\n",
        "# Path to combined concurrent metrics\n",
        "concurrent_metrics_dir = \"/content/concurrent data/metrics\"\n",
        "\n",
        "def combine_metrics(metrics_dir):\n",
        "    combined_list = []\n",
        "    for file in os.listdir(metrics_dir):\n",
        "        if file.endswith(\".csv\"):\n",
        "            path = os.path.join(metrics_dir, file)\n",
        "            df = pd.read_csv(path)\n",
        "            df[\"node\"] = file.replace(\".csv\", \"\")\n",
        "            combined_list.append(df)\n",
        "    combined_df = pd.concat(combined_list, ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "# Combine concurrent metrics\n",
        "combined_concurrent_metrics = combine_metrics(concurrent_metrics_dir)\n",
        "total_rows = combined_concurrent_metrics.shape[0]\n",
        "\n",
        "# Split into 2\n",
        "split_index = math.ceil(total_rows / 3)\n",
        "concurrent_part1 = combined_concurrent_metrics.iloc[:split_index]\n",
        "concurrent_part2 = combined_concurrent_metrics.iloc[split_index:]\n",
        "concurrent_part3 = combined_concurrent_metrics.iloc[split_index:]\n",
        "\n",
        "# Save\n",
        "concurrent_part1.to_csv(\"/content/combined concurrent_metrics_part1.csv\", index=False)\n",
        "concurrent_part2.to_csv(\"/content/combined concurrent_metrics_part2.csv\", index=False)\n",
        "concurrent_part3.to_csv(\"/content/combined concurrent_metrics_part3.csv\", index=False)\n",
        "\n",
        "print(\"Concurrent metrics split into two files:\")\n",
        "print(\"Part 1 shape:\", concurrent_part1.shape)\n",
        "print(\"Part 2 shape:\", concurrent_part2.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFOc3uG0LyE7",
        "outputId": "7923467b-3fbf-40dc-a326-6ce089f5a95b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concurrent metrics split into two files:\n",
            "Part 1 shape: (1434016, 8)\n",
            "Part 2 shape: (2868032, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Part 2 shape:\", concurrent_part3.shape)"
      ],
      "metadata": {
        "id": "b6HCZLu1Nkd5",
        "outputId": "b4b8faf9-669b-43a7-830c-d0dc4f929ecd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 2 shape: (2868032, 8)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPML8GIFqQlBepYM/TMpEBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}